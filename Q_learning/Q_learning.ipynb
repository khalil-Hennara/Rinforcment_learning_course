{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class is the enviroment we use \n",
    "#the openAI enviornment hase some bug so we create this one \n",
    "class FrozenLake:\n",
    "    def __init__(self):\n",
    "        self.Grid=[['S','F','F','F'],['F','H','F','H'],['F','F','F','H'],['H','F','F','G']]\n",
    "        self.pos=None\n",
    "        self.N=4\n",
    "    \n",
    "    #start the game should be called after we create the object\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        this function start the game \n",
    "        reset the bored and the inite position\n",
    "        return 0 wich is the inital state\n",
    "        \"\"\"\n",
    "        self.pos=[0,0]\n",
    "        self.grid=copy.deepcopy(self.Grid)\n",
    "        self.grid[self.pos[0]][self.pos[1]]='*'\n",
    "        return 0\n",
    "    \n",
    "    #to validate the move we wont make it \n",
    "    def _valid_move(self,x,y):\n",
    "        \"\"\"\n",
    "        validation move from user \n",
    "        parameter (x,y):\n",
    "            x:is index of row\n",
    "            y is index of column\n",
    "        return bool:\n",
    "            True if can move to the cell[x][y]\n",
    "            False other wise\n",
    "        \"\"\"\n",
    "        if x<self.N and y<self.N and x>=0 and y>=0:return True\n",
    "        else:return False\n",
    "        \n",
    "    #this one creat the step and return obs,reward,done\n",
    "    def step(self,action):\n",
    "        \"\"\"\n",
    "        this function take an action and reutun the state of game\n",
    "        parameter:\n",
    "            action=range(4)\n",
    "            {0:Up,1:Down,2:right,3:left}\n",
    "        return :(state,reward,done)\n",
    "            state:the new state is in range [0,15]\n",
    "            reward 0 if not finish -1 if he loss 1 if we reach the Gole\n",
    "            done False while not (win or loss)\n",
    "        \"\"\"\n",
    "        reward=-1\n",
    "        done=False\n",
    "        if action==0:\n",
    "            #go Up\n",
    "            if self._valid_move(self.pos[0]-1,self.pos[1]):\n",
    "                if self.grid[self.pos[0]-1][self.pos[1]]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]-1][self.pos[1]]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]-1][self.pos[1]]='*'\n",
    "                self.pos[0]-=1\n",
    "        elif action==1:\n",
    "            #go Down\n",
    "            if self._valid_move(self.pos[0]+1,self.pos[1]):\n",
    "                if self.grid[self.pos[0]+1][self.pos[1]]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]+1][self.pos[1]]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]+1][self.pos[1]]='*'\n",
    "                self.pos[0]+=1\n",
    "        elif action==2:\n",
    "            #go Right\n",
    "            if self._valid_move(self.pos[0],self.pos[1]+1):\n",
    "                if self.grid[self.pos[0]][self.pos[1]+1]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]][self.pos[1]+1]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]][self.pos[1]+1]='*'\n",
    "                self.pos[1]+=1\n",
    "        elif action==3:\n",
    "            #go left\n",
    "            if self._valid_move(self.pos[0],self.pos[1]-1):\n",
    "                if self.grid[self.pos[0]][self.pos[1]-1]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]][self.pos[1]-1]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]][self.pos[1]-1]='*'\n",
    "                self.pos[1]-=1\n",
    "        return (self.pos[0]*self.N+self.pos[1]),reward,done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        print the Bord of Game\n",
    "        \"\"\"\n",
    "        for i in self.grid:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=FrozenLake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 2285.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def exploration_policy(epsilon,state,q_tabel):\n",
    "    \"\"\"\n",
    "    this function is for random action \n",
    "    we use epsilon policy for random actions selection\n",
    "    return integer number from [0,3] which indecate the action \n",
    "    \"\"\"\n",
    "    if np.random.rand()<epsilon:\n",
    "        return np.random.randint(1,4)\n",
    "    else:\n",
    "        return np.argmax(q_tabel[state])\n",
    "\n",
    "    \n",
    "#inite the Q_value Tabel where the row is the state and column is action we can do\n",
    "Q_tabel=np.zeros((16,4))\n",
    "\n",
    "#learning rate for Q_value algorithem \n",
    "lr=0.8\n",
    "\n",
    "#the discount factor \n",
    "gamma=0.95\n",
    "\n",
    "#epsilon for epsilon policy\n",
    "epsilon=1\n",
    "\n",
    "#number of epoch\n",
    "epochs=1000\n",
    "\n",
    "#number of step we can do in each game\n",
    "num_steps=100\n",
    "\n",
    "#the list for save the reward in each game to check how many game we have been win\n",
    "reward_List=[]\n",
    "\n",
    "for episod in tqdm(range(1,epochs)):\n",
    "    #reset the game\n",
    "    state=env.reset()\n",
    "    #acumlater for reward in each game which is 0 or -1 or 1\n",
    "    rewards=0\n",
    "    for j in range(num_steps):\n",
    "        #create an action \n",
    "        action=exploration_policy(epsilon,state,Q_tabel)\n",
    "        \n",
    "        #update epsilon which is decrease while we are learning \n",
    "        # and that becouse in start of traning we wont to explore the environment in random way\n",
    "        # and when we learn somthing we wont to take information from what we learn \n",
    "        epsilon = max(1 - episod / 140, 0.1)\n",
    "        \n",
    "        #make move\n",
    "        new_state,reword,done=env.step(action)\n",
    "        \n",
    "        #implement of the Q_value equation you can check in the obove comment\n",
    "        Q_tabel[state,action]*=(1-lr)\n",
    "        Q_tabel[state,action]+=lr*(reword+gamma*np.max(Q_tabel[new_state]))\n",
    "        \n",
    "        #update state and rewards\n",
    "        state=new_state\n",
    "        rewards+=reword\n",
    "        \n",
    "        #if finish the Game Break the loop and start new game \n",
    "        if done:\n",
    "            break\n",
    "    reward_List.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.743"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(reward for reward in reward_List if reward==1)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8       ,  0.77378094,  0.77378094, -0.26490811],\n",
       "       [-0.8       , -1.        ,  0.81450625,  0.73509189],\n",
       "       [-0.8       ,  0.857375  ,  0.77378094,  0.77378094],\n",
       "       [-0.8       , -0.999936  , -0.40560429,  0.81450625],\n",
       "       [ 0.        ,  0.81450625, -1.        , -0.22621906],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.9025    , -1.        , -1.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -1.        ,  0.857375  , -0.18549375],\n",
       "       [-0.8       ,  0.9025    ,  0.9025    ,  0.81450625],\n",
       "       [ 0.        ,  0.95      , -1.        ,  0.857375  ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -0.99968   ,  0.95      , -0.9999872 ],\n",
       "       [ 0.        , -0.05      ,  1.        ,  0.9025    ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env,q_tabel):\n",
    "    \"\"\"\n",
    "    this is to play a game depending on the Q_tabel that we learn\n",
    "    \"\"\"\n",
    "    state=env.reset()\n",
    "    for i in range(15):\n",
    "        action=np.argmax(q_tabel[state,:])\n",
    "        obs,reward,done=env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            if reward==1:\n",
    "                print('win')\n",
    "                break\n",
    "            else:\n",
    "                print('loss')\n",
    "                break\n",
    "        state=obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', '*', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', '*', 'H']\n",
      "['H', 'F', '*', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', '*', 'H']\n",
      "['H', 'F', '*', '*']\n",
      "win\n"
     ]
    }
   ],
   "source": [
    "play_game(env,Q_tabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we will use openAI gym in to implement Q-learnig algorithem using \n",
    "# Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym \n",
    "import random \n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## browse the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of state is : 500  The number of actions are : 6\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of state is :\",env.observation_space.n,\" The number of actions are :\",env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_tabel=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "Q_tabel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the learnig parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of episod (epoch)\n",
    "episods=50000\n",
    "\n",
    "#the number of steps in each episod \n",
    "steps=100\n",
    "\n",
    "#this for epsilon-greedy policy\n",
    "epsilon=1\n",
    "\n",
    "#learning rate \n",
    "lr=0.7\n",
    "\n",
    "#discount rate \n",
    "gamma=0.618\n",
    "\n",
    "decay_rate=0.01\n",
    "max_epsilon=1\n",
    "min_epsilon=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(epislon,state):\n",
    "   \n",
    "    if np.random.rand()<epsilon:\n",
    "        return np.random.randint(0,5)\n",
    "    else:\n",
    "        return np.argmax(Q_tabel[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_algorithem(env,gamma=0.9,lr=0.8,epsilon=1,episods=20000,num_steps=200):\n",
    "    \n",
    "    reward_List=[]\n",
    "    \n",
    "    Q_tabel=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    \n",
    "    for episod in tqdm.tqdm(range(1,episods)):\n",
    "        #reset the game\n",
    "        state=env.reset()\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            #create an action \n",
    "            action=epsilon_greedy_policy(epsilon,state)\n",
    "\n",
    "            #update epsilon which is decrease while we are learning \n",
    "            # and that becouse in start of traning we wont to explore the environment in random way\n",
    "            # and when we learn somthing we wont to take information from what we learn \n",
    "\n",
    "            #make move\n",
    "            new_state,reword,done,info =env.step(action)\n",
    "\n",
    "            #implement of the Q_value equation you can check in the obove comment\n",
    "            Q_tabel[state,action]*=(1-lr)\n",
    "            Q_tabel[state,action]+=lr*(reword+gamma*np.max(Q_tabel[new_state]))\n",
    "\n",
    "            #update state and rewards\n",
    "            state=new_state\n",
    "\n",
    "            #if finish the Game Break the loop and start new game \n",
    "            if done:\n",
    "                break\n",
    "        reword=0 if reword<0 else 1        \n",
    "        reward_List.append(reword)\n",
    "        epsilon = max(1 - episod / 6000, 0.1)\n",
    "    return reward_List,Q_tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19999/19999 [13:52<00:00, 24.03it/s]\n"
     ]
    }
   ],
   "source": [
    "reword_list,Q_tabel=Q_learning_algorithem(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(reword_list)/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_taxi_game(Q_tabel,env):\n",
    "    state=env.reset()\n",
    "    for i in range(25):\n",
    "        action=np.argmax(Q_tabel[state])\n",
    "        obs,reward,done,info=env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            if reward>0:\n",
    "                print('win')\n",
    "            else:\n",
    "                print('loss')\n",
    "            break\n",
    "        state=obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "play_taxi_game(Q_tabel,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
