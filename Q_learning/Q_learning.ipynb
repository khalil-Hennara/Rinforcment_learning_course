{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm #for visiulization stuff\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class is the enviroment we use \n",
    "class FrozenLake:\n",
    "    def __init__(self):\n",
    "        self.Grid=[['S','F','F','F'],['F','H','F','H'],['F','F','F','H'],['H','F','F','G']]\n",
    "        self.pos=None\n",
    "        self.N=4\n",
    "        self.actions_rep=\"0:up,1:down,2:right,3:left\"\n",
    "    \n",
    "    #start the game should be called after we create the object\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        this function start the game \n",
    "        reset the bored and the inite position\n",
    "        return 0 wich is the inital state\n",
    "        \"\"\"\n",
    "        self.pos=[0,0]\n",
    "        self.grid=copy.deepcopy(self.Grid)\n",
    "        self.grid[self.pos[0]][self.pos[1]]='*'\n",
    "        return 0\n",
    "    \n",
    "    #to validate the move we wont make it \n",
    "    def _valid_move(self,x,y):\n",
    "        \"\"\"\n",
    "        validation move from user \n",
    "        parameter (x,y):\n",
    "            x:is index of row\n",
    "            y is index of column\n",
    "        return bool:\n",
    "            True if can move to the cell[x][y]\n",
    "            False other wise\n",
    "        \"\"\"\n",
    "        if x<self.N and y<self.N and x>=0 and y>=0:return True\n",
    "        else:return False\n",
    "        \n",
    "    #this one creat the step and return obs,reward,done\n",
    "    def step(self,action):\n",
    "        \"\"\"\n",
    "        this function take an action and reutun the state of game\n",
    "        parameter:\n",
    "            action=range(4)\n",
    "            {0:Up,1:Down,2:right,3:left}\n",
    "        return :(state,reward,done)\n",
    "            state:the new state is in range [0,15]\n",
    "            reward 0 if not finish -1 if he loss 1 if we reach the Gole\n",
    "            done False while not (win or loss)\n",
    "        \"\"\"\n",
    "        reward=-1\n",
    "        done=False\n",
    "        if action==0:\n",
    "            #go Up\n",
    "            if self._valid_move(self.pos[0]-1,self.pos[1]):\n",
    "                if self.grid[self.pos[0]-1][self.pos[1]]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]-1][self.pos[1]]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]-1][self.pos[1]]='*'\n",
    "                self.pos[0]-=1\n",
    "        elif action==1:\n",
    "            #go Down\n",
    "            if self._valid_move(self.pos[0]+1,self.pos[1]):\n",
    "                if self.grid[self.pos[0]+1][self.pos[1]]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]+1][self.pos[1]]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]+1][self.pos[1]]='*'\n",
    "                self.pos[0]+=1\n",
    "        elif action==2:\n",
    "            #go Right\n",
    "            if self._valid_move(self.pos[0],self.pos[1]+1):\n",
    "                if self.grid[self.pos[0]][self.pos[1]+1]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]][self.pos[1]+1]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]][self.pos[1]+1]='*'\n",
    "                self.pos[1]+=1\n",
    "        elif action==3:\n",
    "            #go left\n",
    "            if self._valid_move(self.pos[0],self.pos[1]-1):\n",
    "                if self.grid[self.pos[0]][self.pos[1]-1]=='H':\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                elif self.grid[self.pos[0]][self.pos[1]-1]=='G':\n",
    "                    reward=1\n",
    "                    done=True\n",
    "                else:\n",
    "                    reward=0\n",
    "                    done=False\n",
    "                self.grid[self.pos[0]][self.pos[1]-1]='*'\n",
    "                self.pos[1]-=1\n",
    "        return (self.pos[0]*self.N+self.pos[1]),reward,done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        print the Bord of Game\n",
    "        \"\"\"\n",
    "        for i in self.grid:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=FrozenLake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:00<00:00, 1696.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def exploration_policy(epsilon,state,q_tabel):\n",
    "    \"\"\"\n",
    "    this function is for random action \n",
    "    we use epsilon policy for random actions selection\n",
    "    return integer number from [0,3] which indecate the action \n",
    "    \"\"\"\n",
    "    if np.random.rand()<epsilon:\n",
    "        return np.random.randint(1,4)\n",
    "    else:\n",
    "        return np.argmax(q_tabel[state])\n",
    "\n",
    "    \n",
    "#inite the Q_value Tabel where the row is the state and column is action we can do\n",
    "Q_tabel=np.zeros((16,4))\n",
    "\n",
    "#learning rate for Q_value algorithem \n",
    "lr=0.8\n",
    "\n",
    "#the discount factor \n",
    "gamma=0.95\n",
    "\n",
    "#epsilon for epsilon policy\n",
    "epsilon=1\n",
    "\n",
    "#number of epoch\n",
    "epochs=1000\n",
    "\n",
    "#number of step we can do in each game\n",
    "num_steps=100\n",
    "\n",
    "#the list for save the reward in each game to check how many game we have been win\n",
    "reward_List=[]\n",
    "\n",
    "for episod in tqdm(range(1,epochs)):\n",
    "    #reset the game\n",
    "    state=env.reset()\n",
    "    #acumlater for reward in each game which is 0 or -1 or 1\n",
    "    rewards=0\n",
    "    for j in range(num_steps):\n",
    "        #create an action \n",
    "        action=exploration_policy(epsilon,state,Q_tabel)\n",
    "        \n",
    "        #update epsilon which is decrease while we are learning \n",
    "        # and that becouse in start of traning we wont to explore the environment in random way\n",
    "        # and when we learn somthing we wont to take information from what we learn \n",
    "        epsilon = max(1 - episod / 140, 0.1)\n",
    "        \n",
    "        #make move\n",
    "        new_state,reword,done=env.step(action)\n",
    "        \n",
    "        #implement of the Q_value equation you can check in the obove comment\n",
    "        Q_tabel[state,action]*=(1-lr)\n",
    "        Q_tabel[state,action]+=lr*(reword+gamma*np.max(Q_tabel[new_state]))\n",
    "        \n",
    "        #update state and rewards\n",
    "        state=new_state\n",
    "        rewards+=reword\n",
    "        \n",
    "        #if finish the Game Break the loop and start new game \n",
    "        if done:\n",
    "            break\n",
    "    reward_List.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.687"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(reward for reward in reward_List if reward==1)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8       ,  0.77378094,  0.6983373 , -0.26490811],\n",
       "       [-0.8       , -1.        ,  0.65096289,  0.73509189],\n",
       "       [-0.8       ,  0.        ,  0.        ,  0.69758756],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.81450625, -1.        , -0.22621906],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , -0.9984    , -0.8       ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -1.        ,  0.857375  , -0.18549375],\n",
       "       [-0.8       ,  0.9025    ,  0.9025    ,  0.81450625],\n",
       "       [ 0.        ,  0.95      , -0.9984    ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -0.0975    ,  0.95      , -1.        ],\n",
       "       [ 0.        , -0.05      ,  1.        ,  0.9025    ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env,q_tabel):\n",
    "    \"\"\"\n",
    "    this is to play a game depending on the Q_tabel that we learn\n",
    "    \"\"\"\n",
    "    state=env.reset()\n",
    "    for i in range(15):\n",
    "        action=np.argmax(q_tabel[state,:])\n",
    "        obs,reward,done=env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            if reward==1:\n",
    "                print('win')\n",
    "                break\n",
    "            else:\n",
    "                print('loss')\n",
    "                break\n",
    "        state=obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['F', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', 'F', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', 'F', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', '*', 'H']\n",
      "['H', 'F', 'F', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', '*', 'H']\n",
      "['H', 'F', '*', 'G']\n",
      "['*', 'F', 'F', 'F']\n",
      "['*', 'H', 'F', 'H']\n",
      "['*', '*', '*', 'H']\n",
      "['H', 'F', '*', '*']\n",
      "win\n"
     ]
    }
   ],
   "source": [
    "play_game(env,Q_tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
