{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CFNok81b0LT5"
      },
      "outputs": [],
      "source": [
        "#import dependencies library\n",
        "\n",
        "# the enviornment library\n",
        "import gym\n",
        "\n",
        "#the AI framework \n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras \n",
        "import numpy as np\n",
        "\n",
        "#just for ploting stuf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKbgl8ZP0X0p",
        "outputId": "c10f3b9a-0ca1-4dfb-d024-c11bd950bb07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unrar\n",
            "  Downloading unrar-0.4-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying crazy_climber.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying elevator_action.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying gravitar.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying keystone_kapers.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from HC ROMS/BY ALPHABET (PAL)/H-R/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying mr_do.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying pacman.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying jamesbond.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying koolaid.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying montezuma_revenge.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying star_gunner.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying time_pilot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying up_n_down.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying sir_lancelot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying amidar.bin from HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying asteroids.bin from HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying defender.bin from HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying enduro.bin from HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying hero.bin from HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying journey_escape.bin from HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying kung_fu_master.bin from HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying lost_luggage.bin from HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying ms_pacman.bin from HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying pitfall.bin from HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying skiing.bin from HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying surround.bin from HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying trondead.bin from HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying venture.bin from HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying video_pinball.bin from HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying wizard_of_wor.bin from HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying assault.bin from HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKi_mjQj0LUT"
      },
      "source": [
        "After we import dependecies we will creat our enviornment using **gym** library and we chose the brakout game as the playground game to worke on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CGrdTJn20LUb"
      },
      "outputs": [],
      "source": [
        "# make the environment we chose breakout game to try on.\n",
        "environment=gym.make(\"Breakout-v4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQK-4DOO0LUe"
      },
      "source": [
        "lets explore the environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt1Rf5Nt0LUg",
        "outputId": "738926ab-c503-4c2a-98f2-3f2e4c84f097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
          ]
        }
      ],
      "source": [
        "print(environment.unwrapped.get_action_meanings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVqOaA6R0LUm",
        "outputId": "04a064de-a21c-427c-94d1-8957961edafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of action is :(4) and which is the number of output of Neural Network\n"
          ]
        }
      ],
      "source": [
        "# the actions space represent the output of the Neural Network \n",
        "action_space=environment.action_space.n\n",
        "print(\"The number of action is :({}) and which is the number of output of Neural Network\".format(action_space))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "yBgTnn1O0LUo",
        "outputId": "48e42d17-794b-4f07-fb19-4d203a88633e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3cc3937490>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "state=environment.reset()\n",
        "plt.imshow(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeGtrbS60LUp"
      },
      "source": [
        "The First step is make some preprocessing to each frame, where want change the **RGB** format to **gray** fromat and that decrease the confusing in the network and we will cut the top of frame which represent the reword which change and may confuse the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sQN0EhwY0LUr"
      },
      "outputs": [],
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    This class is Warpper where we make sure that the game is runing, becouse in some environment in gym library should \n",
        "    start the game by action \" 1 \" in most of the game so by doing this we make sure that the game is roling.\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "        \n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "            obs, _, done, _ = self.env.step(2)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=FireResetEnv(environment)"
      ],
      "metadata": {
        "id": "TDnUvPLo0iKD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "offBZv7B0LUt",
        "outputId": "e3ac36af-6495-45c3-863b-c851baee1f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,686,180\n",
            "Trainable params: 1,686,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#this model is as recomende from paper \"Mnih 2015, Human-level control through deep reinforcement learning\"\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Conv2D(32,8,strides=4,padding='valid',activation='relu',input_shape=[84,84,4]),\n",
        "    keras.layers.Conv2D(64,4,strides=2,padding='valid',activation='relu'),\n",
        "    keras.layers.Conv2D(64,3,strides=1,padding='valid',activation='relu'),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512,activation='relu'),\n",
        "    keras.layers.Dense(action_space)\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqApat9p0LUv"
      },
      "source": [
        "Now lets put all togather in one class "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hO-7Y3430LUw"
      },
      "outputs": [],
      "source": [
        "class Fixed_DQN:\n",
        "    def __init__(self,environment,model,discount_factor=0.95,learning_rate=0.00025,episode=10000,batch_size=32,replay_size=1000000,staked_frame=4,fixed_epsilon=None,model_checkpoint='DQN_wights.h5',up=25,down=10,left=5,right=5,update_steps=100):\n",
        "        #the environment simulator \n",
        "        self.env=environment\n",
        "#---->  hear we made the first change where we have 2 model instead of one model the online_model and target_model \n",
        "#---->  the first one it's using for compute Q_value function at each time step while the other to just compute the target.\n",
        "        self.online_model=model\n",
        "#---->  the second model is the target model have the same structure of the online model and the same inial parameter.\n",
        "        self.target_model=tf.keras.models.clone_model(model)\n",
        "        self.target_model.set_weights(model.get_weights())\n",
        "        # the discout factor in the Q-learning algorithem\n",
        "        self.discount_factor=discount_factor\n",
        "        # the number of episod to run as the number of epochs in supervised learning \n",
        "        self.episodes=episode\n",
        "#---->  onather update to the previouse alogorithem where we need parameter that difine the update rate for the target model.\n",
        "        self.update_rate=update_steps\n",
        "    \n",
        "        self.batch_size=batch_size\n",
        "        # the memory where we save the observations for training \n",
        "        self.replay_buffer=deque(maxlen=replay_size)\n",
        "        # the deque used for state creation where we stacked last 4 frames togather \n",
        "        self.stacked_blocks=deque(maxlen=staked_frame)\n",
        "        # if we want to stack more or less frame togather in other world if we want the state more complex or simpler.\n",
        "        self.stacked_size=staked_frame\n",
        "        # if we want the epsilon parameter constant during the training else the parameter will change during the training.\n",
        "        self.fixed_epsilon=fixed_epsilon\n",
        "        # the action space it's represent the number neuron in the output layer becouse Q function is maping from state to actions. \n",
        "        self.action_space=environment.action_space.n\n",
        "        # it's rewards list where we save the reward from each episod ( from each epochs )\n",
        "        self.rewards_list=[]\n",
        "        # the loss function using to compute the gradiant of the model.\n",
        "        self.loss_function=tf.losses.mean_squared_error\n",
        "        # the optimizer using to update the parameter.\n",
        "        self.optimizer=tf.optimizers.RMSprop(learning_rate=learning_rate,momentum=0.95)\n",
        "        # it's the file path, where we save the model weights\n",
        "        self.model_checkpoint=model_checkpoint\n",
        "        # these four parameter we use for croping the edges of the frame  \n",
        "        self.up=up\n",
        "        self.down=-down\n",
        "        self.left=left\n",
        "        self.right=-right\n",
        "    \n",
        "    def _preprocess_frame(self,frame):\n",
        "        \"\"\"\n",
        "        this function make preprocessing step for each frame from the game.\n",
        "        parameter: \n",
        "            -frame: is a row frame with [210*64*3] size which provided by the environment, after we make an action that change the world\n",
        "            the environmnt brovide us by new observation after apply that action.\n",
        "        return :\n",
        "            image: processing frame where we cut some of border and keep the play area and change the image to gray scale \n",
        "            then resize it to [84,84] shape and finaly normalize pixel value to become in the range [0-1] \n",
        "        \"\"\"\n",
        "        # cut the porder and keep the play area\n",
        "        image=frame[self.up:self.down,self.left:self.right,:]\n",
        "        image=tf.image.rgb_to_grayscale(image)\n",
        "        image=tf.image.resize(image,[84,84])\n",
        "        # reshape becouse it's tensor [84,84,1] and we want [84,84] for make the stacking operation easy\n",
        "        image=tf.reshape(image,[84,84])\n",
        "        # normalizing step\n",
        "        image=image/255.0\n",
        "        return image\n",
        "    \n",
        "    def ploting_function_one(frame):\n",
        "        \"\"\"\n",
        "        is the same as preprocessing function but we use it for viualization stuf.\n",
        "        \"\"\"\n",
        "        image=frame[25:,:,:]\n",
        "        image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "        image=cv2.resize(image,(85,85))\n",
        "        return image\n",
        "    \n",
        "    \n",
        "    def _state_creator(self,frame,is_new_episod):\n",
        "        \"\"\"\n",
        "        this function we stack last 4 frame togather to perform on state and that give us some intuition about vilocity.\n",
        "        parameter :\n",
        "            -frame :which is the game observation.\n",
        "            -is_new_episod: boolean parameter that check if there is no previous frame and that happend in bragning of each episod.\n",
        "        \n",
        "        return :\n",
        "            state: tensor with [84,84,4] shape which represent the state of our world.\n",
        "        \"\"\"\n",
        "        # first preprocessing the frame.\n",
        "        image=self._preprocess_frame(frame)\n",
        "        # if it's new episod then stack the first frame four time \n",
        "        if is_new_episod:\n",
        "            for i in range(self.stacked_size):\n",
        "                self.stacked_blocks.append(image)\n",
        "                \n",
        "        # just push the last frame so the first frame get out from the deque\n",
        "        else:\n",
        "            self.stacked_blocks.append(image)\n",
        "            \n",
        "        # stacked the frames togather in one tensor [84,84,4]\n",
        "        state=tf.stack(self.stacked_blocks,axis=2)\n",
        "        \n",
        "        return state\n",
        "    \n",
        "    def _sample_experiences(self):\n",
        "        \"\"\"\n",
        "        This function is use to sample batch from our memory.\n",
        "        \n",
        "        return:\n",
        "            -states:it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
        "            -actions: it's 2D array [batch_size,action] it's the action in each state in our batch\n",
        "            -rewards: it's 2D array [batch_size,reward] it's the reward in each state in out batch\n",
        "            -dones : it's 2D array [batch_size, done ] where done is boolean value help us to compute the target.\n",
        "            -next_state: it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
        "        \"\"\"\n",
        "        # radnom indices from the replay buffer\n",
        "        indices=np.random.randint(len(self.replay_buffer),size=self.batch_size)\n",
        "        batch=[self.replay_buffer[index] for index in indices]\n",
        "        # we combine the experiance togather where the buffer have tuple like this (state,action,reward,done,next_state)\n",
        "        states,actions,rewords,dones,next_states=[np.array([experiance[field_index] for experiance in batch]) for field_index in range(5)]\n",
        "        return states,actions,rewords,dones,next_states\n",
        "    \n",
        "    \n",
        "    def _epsilon_greedy_policy(self,state,epsilon):\n",
        "        \"\"\"\n",
        "        Is the epislon greedy policy whre we use epsilon value to chose an action\n",
        "        where we want to palance the exploration and explotation as possiable as we can.\n",
        "    \n",
        "        we pick random value alpha.\n",
        "            -if alpha < epsilon : chose random action\n",
        "             else argmax Q(state) for all action.\n",
        "        parameter :\n",
        "            -state: is the current state.\n",
        "            -epsilon : is the value of threshould between [0,1]\n",
        "        return :\n",
        "            the number of action to make \n",
        "    \n",
        "        \"\"\"\n",
        "        if np.random.rand()<epsilon:\n",
        "            return np.random.randint(self.action_space)\n",
        "        else:\n",
        "#---->      hear we use the online model for prediction of best action.\n",
        "            Q_values=self.online_model.predict(state[np.newaxis])\n",
        "            return tf.argmax(Q_values[0])\n",
        "    \n",
        "    def _play_one_step(self,state,epsilon):\n",
        "        \"\"\"\n",
        "        This function make an action and get the observation from the world.\n",
        "        we make an action and then get the full obesrvation from our environment aplay state_create function to the our \n",
        "        observation(frame) and then save it in the memory for training step.\n",
        "    \n",
        "        parameter :\n",
        "            -state : it's the current state.\n",
        "            -epsilon: it's the thresould we use for action selection\n",
        "        return :\n",
        "            next_state: tensore [84,84,4] block represent the last 4 frame.\n",
        "            reward : it's float number the reward we get after we do some action . \n",
        "            done : it's boolean refer if the game finish or not \n",
        "            info : it's dictionary have the counter of lives.\n",
        "        \"\"\"\n",
        "          \n",
        "        action=self._epsilon_greedy_policy(state,epsilon)\n",
        "        next_state,reward,done,info=self.env.step(action)\n",
        "        # we do that becouse we need True done value for computing the target for the model.\n",
        "        if info['ale.lives']< 5:\n",
        "            done=True\n",
        "        stacked_next_state=self._state_creator(next_state,False)\n",
        "        self.replay_buffer.append((state,action,reward,done,stacked_next_state))\n",
        "        return stacked_next_state,reward,done,info\n",
        "    \n",
        "    def _training_step(self):\n",
        "        \"\"\"\n",
        "        This function is the responsible for doing one training step where we sample one batch from the replay buffer \n",
        "        then we push this batch in our model for forward step and apply gradiant with optimizer by hand.\n",
        "        \n",
        "        return :\n",
        "            None , where it's apply the change inplace for model parameter.\n",
        "        \"\"\"\n",
        "        # get the batch from the replay_buffer (our memory)\n",
        "        states,actions,rewards,dones,next_states=self._sample_experiences()\n",
        "#---->  the online_Q(state,a)=online_Q(state,a) + alpha [ reward + gamma * max (target_Q(next_state,a) for all a) - online_Q(state,a)] \n",
        "        next_Q_values=self.target_model.predict(next_states)\n",
        "        max_next_Q_values=np.max(next_Q_values,axis=1)\n",
        "        # compute the target which is [reward +  gamma * max(target_Q(next_state,a) over a)] \n",
        "        target_Q_values=rewards+(1-dones)*self.discount_factor*max_next_Q_values\n",
        "        \n",
        "        #this mask it's important for vanishing the value of actions that we don't used it's 2D array with 1 if action i was picked and zero othewise.\n",
        "        mask = tf.one_hot(actions, self.action_space)\n",
        "\n",
        "        #start the gradient recording to compute the gradient for our model.\n",
        "        with tf.GradientTape() as tape:\n",
        "#---->      compute the Q function for current state (hear for the whole batch).\n",
        "            all_Q_values=self.online_model(states)\n",
        "            #hear we use the mask so we reduce the result for just the action we picked.\n",
        "            Q_values=tf.reduce_sum(all_Q_values*mask,axis=1,keepdims=True)\n",
        "            #computer the loss function with the target we compute erlaier.\n",
        "            loss=tf.reduce_mean(self.loss_function(target_Q_values,Q_values))\n",
        "#---->      compute the gradient for our  online_model parameter.\n",
        "        gradiants=tape.gradient(loss,self.online_model.trainable_variables)\n",
        "#---->  apply the optimization step using the gradient we compute for the online_model\n",
        "        self.optimizer.apply_gradients(zip(gradiants,self.online_model.trainable_variables))\n",
        "        \n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        This function represent the training loop function where we are trying to fit model to produce the best function\n",
        "        according to the data we have.\n",
        "        \n",
        "        \"\"\"\n",
        "        for episode in tqdm.tqdm(range(self.episodes)):\n",
        "            state=self.env.reset()\n",
        "            stacked_state=self._state_creator(state,True)\n",
        "            rewards=0\n",
        "            epsilon=max(1-episode/600,0.1)\n",
        "            while True:\n",
        "                stacked_state,reward,done,info=self._play_one_step(stacked_state,epsilon)\n",
        "                rewards+=reward\n",
        "                if done :\n",
        "                    if not (episode%10) and len(self.rewards_list):\n",
        "                        self.online_model.save_weights(self.model_checkpoint)\n",
        "                        print(\"The best score for last 10 episode is: {} and the worst one is: {}\".format(max(self.rewards_list[-10:]),min(self.rewards_list[-10:])))\n",
        "                    self.rewards_list.append(rewards)\n",
        "                    break\n",
        "                # this step to let the replay buffer has some experiance\n",
        "                if episode>50:\n",
        "                    self._training_step()\n",
        "#----->         hear where update the target model after 100 episod by set the parameter as online model.\n",
        "                if episode%self.update_rate:\n",
        "                    self.target_model.set_weights(self.online_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent=Fixed_DQN(env,model)"
      ],
      "metadata": {
        "id": "3raSUAkL05Nc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.fit()"
      ],
      "metadata": {
        "id": "_NK7gBnN1R9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3HGV5cDu16iX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "Fixed_DQN.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}