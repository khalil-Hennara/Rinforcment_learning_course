{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b1hab-IwT63S"
      },
      "outputs": [],
      "source": [
        "#import dependencies library\n",
        "\n",
        "# the enviornment library\n",
        "import gym\n",
        "\n",
        "#the AI framework \n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras \n",
        "import numpy as np\n",
        "\n",
        "#just for ploting stuf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars\n"
      ],
      "metadata": {
        "id": "p7MiDVcVUAf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2or8QsRUT63h"
      },
      "source": [
        "After we import dependecies we will creat our enviornment using **gym** library and we chose the brakout game as the playground game to worke on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1YwddCN_T63m"
      },
      "outputs": [],
      "source": [
        "# make the environment we chose breakout game to try on.\n",
        "environment=gym.make(\"Breakout-v4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4NUcRyNT63o"
      },
      "source": [
        "lets explore the environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ALKiMJkT63p",
        "outputId": "cb86431f-9ef3-4be7-9a45-fc2b85ff1cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
          ]
        }
      ],
      "source": [
        "print(environment.unwrapped.get_action_meanings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foy1f2_FT63s",
        "outputId": "b49ca641-e8f1-4dc4-df99-2405ad27407b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of action is :(4) and which is the number of output of Neural Network\n"
          ]
        }
      ],
      "source": [
        "# the actions space represent the output of the Neural Network \n",
        "action_space=environment.action_space.n\n",
        "print(\"The number of action is :({}) and which is the number of output of Neural Network\".format(action_space))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "EjxhCxN8T63u",
        "outputId": "dfdb2fcf-e958-47ca-d0d3-57299fc32961"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8749673110>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "state=environment.reset()\n",
        "plt.imshow(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5BMirRuT63v"
      },
      "source": [
        "The First step is make some preprocessing to each frame, where want change the **RGB** format to **gray** fromat and that decrease the confusing in the network and we will cut the top of frame which represent the reword which change and may confuse the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j4HYpsnMT63x"
      },
      "outputs": [],
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    This class is Warpper where we make sure that the game is runing, becouse in some environment in gym library should \n",
        "    start the game by action \" 1 \" in most of the game so by doing this we make sure that the game is roling.\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "        \n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "            obs, _, done, _ = self.env.step(2)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=FireResetEnv(environment)"
      ],
      "metadata": {
        "id": "4e5jVE9vUGTJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_SGosBtT63z",
        "outputId": "68e9229f-ab46-4607-b457-51c1f1f9091b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,686,180\n",
            "Trainable params: 1,686,180\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#this model is as recomende from paper \"Mnih et al., 2015; van Hasselt et al., 2015\"\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Conv2D(32,8,strides=4,padding='valid',activation='relu',input_shape=[84,84,4]),\n",
        "    keras.layers.Conv2D(64,4,strides=2,padding='valid',activation='relu'),\n",
        "    keras.layers.Conv2D(64,3,strides=1,padding='valid',activation='relu'),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512,activation='relu'),\n",
        "    keras.layers.Dense(action_space)\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxmPLz16T631"
      },
      "source": [
        "In this Notebook we will implement Double DQN with proportional prioritization from **[Schaul etc,, 2016](link)** as it's outperforms the DDQN with uniform propabiltiy for transition in the replay buffer.\n",
        "\n",
        "*Experience replay* as we show in previous lecture that if we using replay buffer that break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update.\n",
        "This was demonstrated in the Deep Q-Network (DQN) algorithm (Mnih et al., 2013; 2015), which\n",
        "stabilized the training of a value function, represented by a deep neural network, by using experience\n",
        "replay\n",
        "\n",
        "More specifically, experiences are considered **important** if they are likely to lead to\n",
        "fast learning progress,While this measure is not directly accessible,a reasonable proxy is the **magnitude of a transition’s TD error δ**, which indicates how **surprising** or **unexpected** the transition is: specifically, how far the value is from its next-step bootstrap estimate.\n",
        "\n",
        "so this idea it's almost the same as boosting where we creat a new classifire for the examples we get them wrong and train on thouse miss predict examples, but this may lead to over-fit becouse we train on some examples more than other.\n",
        "\n",
        "so as boosting this will happen for **greedy TD-error** First, to avoid expensive sweeps over\n",
        "the entire replay memory, TD errors are only updated for the transitions that are replayed. One\n",
        "consequence is that transitions that have a low TD error on first visit may not be replayed for a long time.\n",
        "Finally, greedy prioritization focuses on a\n",
        "small subset of the experience: errors shrink slowly, especially when using function approximation,\n",
        "meaning that the initially high error transitions get replayed frequently. This lack of diversity that\n",
        "makes the system prone to over-fitting.\n",
        "\n",
        "To overcome these issues, we introduce a **stochastic sampling** method that interpolates between\n",
        "pure **greedy prioritization** and **uniform random sampling**. We ensure that the probability of being\n",
        "sampled is monotonic in a transition’s priority, while guaranteeing a non-zero probability even for\n",
        "the lowest-priority transition. Concretely, we define the probability of sampling transition i as\n",
        "\n",
        "$P(i) = \\frac{p_i^\\alpha}{\\sum_k p^\\alpha}$\n",
        "\n",
        "where $p_i > 0 $ is the priority of transition i. The exponent $ α $ determines how much prioritization is\n",
        "used, with $α = 0$ corresponding to the uniform case. \n",
        "so $α$ become another hyperparameter we need to tune but for atari game as the Author of the paper find $α = 0.6$\n",
        "is the best for this problem but for other problem we should tune it's value for the best approximation. \n",
        "**<p style=\"color:red;\">Important Note:</p>**\n",
        "$p_i > 0 $ is the **TD-error** which is from the Definition $[R_j+ \\gamma_j Q(s_{j+1},argmax_a Q(s_{j+1},a),\\theta^-)-Q(s_j,a_j,\\theta^+)]$\n",
        "\n",
        "but we will use the erorr form the Neural Network which is the squred of the **TD-erorr** becouse the target of our model is $[R_j+ \\gamma_j Q(s_{j+1},argmax_a Q(s_{j+1},a),\\theta^-)]$ and that is the same as before without the last tearm which is the output of the model, we add small positive constant $\\epsilon$ that prevents the edge-case of transitions not being revisited once their error is zero\n",
        "\n",
        "\n",
        "Now after we prioritized the transition, the transition with Hight priority will be sampled more often until the erorr of this transition go down and that may laid to over-fit.\n",
        "\n",
        "We can correct this bias by using importance-sampling (IS) weights\n",
        "\n",
        "$w_i=(N*P(i))^-\\beta$\n",
        "\n",
        "where $N$ is the Number of samples in the memory and $P(i)$ is the probability of transition $i$\n",
        "\n",
        "$β$ is a hyperparameter that controls how much we want to compensate for the importance sampling bias (0 means not at\n",
        "all, while 1 means entirely). In the paper, the authors used $β = 0.4$ at the beginning of\n",
        "training and linearly increased it to β = 1 by the end of training. Again, the optimal value will depend on the task\n",
        "\n",
        "after we highlight the important point of the algorithm lets start the implementation.\n",
        "\n",
        "## First: The Sum-Tree:\n",
        "\n",
        "The **sum-tree** data structure used here is very similar in spirit to the\n",
        "array representation of a binary heap. However, instead of the usual heap property, the value of\n",
        "a parent node is the sum of its children. Leaf nodes store the transition priorities and the internal\n",
        "nodes are intermediate sums, with the parent node containing the sum over all priorities, $p_{total}$ . This\n",
        "provides a efficient way of calculating the cumulative sum of priorities, allowing O(log N ) updates\n",
        "and sampling. To sample a minibatch of size k, the range $[0, p_{total}]$ is divided equally into k ranges.\n",
        "\n",
        "Next, a value is uniformly sampled from each range. Finally the transitions that correspond to each\n",
        "of these sampled values are retrieved from the tree\n",
        "\n",
        "beside this structure we will add other proberty rolling data structure, whene the data full it's rolling to the first element and overwrite it.\n",
        "\n",
        "a good article explaine the Sum Tree : [Introduction to Sum Tree](https://www.fcodelabs.com/2019/03/18/Sum-Tree-Introduction/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mBE-HU9NT64C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class SumTree:\n",
        "    \"\"\"\n",
        "    This class is implementation to Sum Tree data structure this data structure is compleat binary Tree \n",
        "    and is the same as Priority Qeue where the root node contain the sum of all leaf node in the Tree \n",
        "    using this data structure reduce the update time to O(log(n)) and find the sum to O(1).\n",
        "    The root node have index = 1 and keep the 0 index unuse and that for make the left children is 2*parent_index and that just for simplicity.\n",
        "    we implement this data structure for using in Proportional Prioritization in the Prioritized Experience Replay from deepmid\n",
        "    \n",
        "    Attributes: \n",
        "       size : represent the size of buffer \n",
        "       tree : it's the binary Tree use to the priority which need to get the sum and update.\n",
        "       data : where we save the information \n",
        "       current_pos: is the pointer to the current postion of the data \n",
        "       n_entries : number of information we have.\n",
        "       max_prio :  this is not relate to the Sum Tree but we need to trake the Max priority for using in our algorithm\n",
        "                   and by doing this we reduce the time to O(1)\n",
        "    method :\n",
        "        -add : using this method to add transition to data list and priority to the tree and updat the tree by 'Bubbel up' as the Priority Qeue \n",
        "        -update :  just like 'Bubbel up' where add the priority to empty slot in the tree and then update it's parent until we rich the root node.\n",
        "        -total  : return the root value which is the sum of all leaves in the tree\n",
        "        -get_sample : travers in tree to find the priority and the data linked to that priority.\n",
        "    \"\"\"\n",
        "    def __init__(self,buffer_size):\n",
        "        self.size=buffer_size\n",
        "        self.tree=[0]*2*buffer_size\n",
        "        self.data=[]\n",
        "        self.current_pos=0\n",
        "        self.n_entries=0\n",
        "        self.max_prio=1\n",
        "    \n",
        "    def add(self,transition,priority):\n",
        "        \"\"\"\n",
        "        in This function we add a new data to the tree \n",
        "        parameter: \n",
        "            transition: is data we will save it \n",
        "            priority : is the value we add to the tree and then update the sum over the whole tree.\n",
        "        return :\n",
        "            None, working inplace.\n",
        "        \"\"\"\n",
        "        if self.n_entries<self.size:    \n",
        "            self.n_entries += 1\n",
        "            self.data.append(transition)\n",
        "            \n",
        "            self.current_pos = (self.current_pos + 1) % self.size\n",
        "            \n",
        "            node_index=self.current_pos + self.size\n",
        "            \n",
        "            \n",
        "            self.update(node_index,priority)\n",
        "            \n",
        "        else:\n",
        "            self.data[self.current_pos]=transition\n",
        "            \n",
        "            node_index=self.current_pos + self.size\n",
        "        \n",
        "            self.update(node_index,priority)\n",
        "        \n",
        "            self.current_pos = (self.current_pos + 1) % self.size\n",
        "        \n",
        "            \n",
        "    def total(self):\n",
        "        \"\"\"\n",
        "        This function return the sum of all leaves in the tree which is root node\n",
        "        \"\"\"\n",
        "        return self.tree[1]\n",
        "    \n",
        "    def update(self,node,priority):\n",
        "        \"\"\"\n",
        "        This function update the nodes in the tree adter we add new node or update a value of exist node \n",
        "        parameter :\n",
        "            -node : is the index of node we want to update it's value which is leave\n",
        "            -priority : is the value to add to tree\n",
        "        \"\"\"\n",
        "        \n",
        "        self.max_prio=max(self.max_prio,priority)\n",
        "        \n",
        "        different=priority-self.tree[node]\n",
        "        \n",
        "        self.tree[node] = priority\n",
        "        \n",
        "        parent = node // 2\n",
        "        \n",
        "        self.tree[parent] += different\n",
        "        \n",
        "        while parent !=1:\n",
        "            parent =parent // 2\n",
        "            self.tree[parent] += different\n",
        "    \n",
        "    \n",
        "    def _retrieve(self,index,random_value):\n",
        "        \"\"\"\n",
        "        This function is for find the right value in the tree after we provide a random value \n",
        "        \"\"\"\n",
        "        left_chield = 2*index\n",
        "        right_chield= 2*index + 1\n",
        "        \n",
        "        if left_chield >= 2*self.size:\n",
        "            return index\n",
        "        \n",
        "        if self.tree[left_chield]>=random_value:\n",
        "            return self._retrieve(left_chield,random_value)\n",
        "        else :\n",
        "            return self._retrieve(right_chield,random_value-self.tree[left_chield])\n",
        "    \n",
        "    def get_sample(self,random_value):\n",
        "        \"\"\"\n",
        "        In this function we travers the whole tree from the root node to the bottom of the tree using the retrieve method \n",
        "        \"\"\"\n",
        "        index=self._retrieve(0,random_value)\n",
        "        \n",
        "        priority=self.tree[index]\n",
        "        \n",
        "        data_index=index-self.size-1\n",
        "        try:\n",
        "            transition=self.data[data_index]\n",
        "        except:\n",
        "            print(\"The error happen hear :\",data_index)\n",
        "            print(\"The index is :\",index)\n",
        "            print(\"Priority :\",priority)\n",
        "        return (transition,priority,index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIWi3JQfT64F"
      },
      "source": [
        "## Secand: Memory\n",
        "Now we will implement the The Memory class that depend on the Sum Tree data structure which the replay buffer.\n",
        "the replay buffer now is SumTree with some other functions the most important one is **sample experiances** function \n",
        "in this function we sample the data from the memory as we explaine a bove we also calculate the sample weights as in Equation $w_i=(N*P(i))^-\\beta$ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zAabtYxCT64G"
      },
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    \"\"\"\n",
        "    This class is used to creat our replay buffer to use in the *Prioritized Experience Replay* \n",
        "    \n",
        "    Attributes:\n",
        "        -size : the replay buffer size.\n",
        "        -tree : is the Sum Tree data structure .\n",
        "        -alpha : the parameter using in the the stochastic probability priority=(p_i+epsilon)**alpha \n",
        "        -epsilon :  is positive small value added to the error \n",
        "        -beta : is the prameter used to calculate the weigths\n",
        "        -BETA_0 : is the statrting value of the beta \n",
        "        -BETA_FRAMS : is used for update the beta factor during the training.\n",
        "    \n",
        "    method:\n",
        "        -add : used to add data to the tree.\n",
        "        -sample_experiences : this function is used to sample experiences from the tree\n",
        "        -update : update the priority in the tree after training one batch of data\n",
        "    \"\"\"\n",
        "    BETA_0=0.4\n",
        "    BETA_FRAMES=1e7\n",
        "    \n",
        "    def __init__(self,size):\n",
        "        self.size=size\n",
        "        self.tree=SumTree(size)\n",
        "        self.alpha=0.6\n",
        "        self.epsilon=1e-5\n",
        "        self.beta=0.4\n",
        "    \n",
        "    def _update_beta(self):\n",
        "        \"\"\"\n",
        "        This function update the beta parameter from 0.4 to 1 after 1e7 frame \n",
        "        \"\"\"\n",
        "        tmp=self.beta+(1-self.BETA_0)/self.BETA_FRAMES\n",
        "        self.beta=min(1,tmp)\n",
        "        \n",
        "        \n",
        "    def add(self,transition):\n",
        "        \"\"\"\n",
        "        this function add a new data to the memory with priority= max pi \n",
        "        and that is the reson, we keep track of the Max priority in the Sum Tree class\n",
        "        \n",
        "        parameter :\n",
        "            -transition : is the data we add (state, action ,reward, done ,next_state ) we add this record to the Tree with \n",
        "                          max priority , and that ensure that, this transition will train on it at least once.\n",
        "        \"\"\"\n",
        "        priority=self.tree.max_prio\n",
        "        self.tree.add(transition,priority)\n",
        "        \n",
        "    def sample_experiences(self,batch_size):\n",
        "        \"\"\"\n",
        "        This function used to sample data from the memory \n",
        "        parameter :\n",
        "            - batch_size :  the number of training example we will use for training.\n",
        "        return :\n",
        "            - batch : is the data we sample from the memory list with shape [batch_size,1] where each value is (state,action,reward,done,next_state)\n",
        "            - weights : is the importance sampling (IS) weights \n",
        "            - indices :  the index of each training example used for train for used later in update the priority in the Tree.\n",
        "        \"\"\"\n",
        "        batch_data=[]\n",
        "        indecis=[]\n",
        "        priorites=[]\n",
        "        p_total=self.tree.total()\n",
        "        segment=p_total/batch_size\n",
        "        self._update_beta()\n",
        "        for i in range(batch_size):\n",
        "            low = segment * i\n",
        "            hight = segment * (i + 1)\n",
        "            \n",
        "            value =np.random.uniform(low, hight)\n",
        "            \n",
        "            transition,priority,index=self.tree.get_sample(value)\n",
        "            batch_data.append(transition)\n",
        "            indecis.append(index)\n",
        "            priorites.append(priority)\n",
        "            \n",
        "        priorites=np.array(priorites)/p_total\n",
        "        \n",
        "        weights=np.power(self.tree.n_entries*priorites,-self.beta)\n",
        "        weights /= np.max(weights)\n",
        "        \n",
        "        return (batch_data,weights,indecis)\n",
        "        \n",
        "    def update(self,indices,errors):\n",
        "        \"\"\"\n",
        "        This function receive the errors from the model and calculate the priority then calls the update function \n",
        "        from the Sum Tree opject on each node used for training.\n",
        "        parameter : \n",
        "            - indices : the index of each training example used for train.\n",
        "            -error : is the error for each transition used in training model.\n",
        " \n",
        "        \"\"\"\n",
        "        priorites=(np.abs(errors)+self.epsilon)**self.alpha\n",
        "        for index,priority in zip(indices,priorites):\n",
        "            self.tree.update(index,priority)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "63XPIOiuT64Q"
      },
      "outputs": [],
      "source": [
        "#----->  The name of the class we change from The Double_DQN to Agent I am just Kiding...\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self,environment,model,discount_factor=0.95,learning_rate=0.00025/4,episode=10000,batch_size=32,replay_size=1000000,staked_frame=4,fixed_epsilon=None,model_checkpoint='DQN_wights_with_PrioritizedReplay.h5',up=25,down=10,left=5,right=5,update_steps=50):\n",
        "        #the environment simulator \n",
        "        self.env=environment\n",
        "        # hear we made the first change where we have 2 model instead of one model the online_model and target_model \n",
        "        #  the first one it's using for compute Q_value function at each time step while the other to just compute the target.\n",
        "        self.online_model=model\n",
        "        # the second model is the target model have the same structure of the online model and the same inial parameter.\n",
        "        self.target_model=keras.models.clone_model(model)\n",
        "        self.target_model.set_weights(model.get_weights())\n",
        "        # the discout factor in the Q-learning algorithem\n",
        "        self.discount_factor=discount_factor\n",
        "        # the number of episod to run as the number of epochs in supervised learning \n",
        "        self.episodes=episode\n",
        "        # onather update to the previouse alogorithem where we need parameter that difine the update rate for the target model.\n",
        "        self.update_rate=update_steps\n",
        "    \n",
        "        self.batch_size=batch_size\n",
        "#---->  the memory where we save the observations for training we now use our memory.\n",
        "        self.replay_buffer=Memory(replay_size)\n",
        "        # the deque used for state creation where we stacked last 4 frames togather \n",
        "        self.stacked_blocks=deque(maxlen=staked_frame)\n",
        "        # if we want to stack more or less frame togather in other world if we want the state more complex or simpler.\n",
        "        self.stacked_size=staked_frame\n",
        "        # if we want the epsilon parameter constant during the training else the parameter will change during the training.\n",
        "        self.fixed_epsilon=fixed_epsilon\n",
        "        # the action space it's represent the number neuron in the output layer becouse Q function is maping from state to actions. \n",
        "        self.action_space=environment.action_space.n\n",
        "        # it's rewards list where we save the reward from each episod ( from each epochs )\n",
        "        self.rewards_list=[]\n",
        "        # the loss function using to compute the gradiant of the model.\n",
        "        self.loss_function=tf.losses.mean_squared_error\n",
        "        # the optimizer using to update the parameter.\n",
        "        self.optimizer=tf.optimizers.RMSprop(learning_rate=learning_rate,momentum=0.95)\n",
        "        # it's the file path, where we save the model weights\n",
        "        self.model_checkpoint=model_checkpoint\n",
        "        # these four parameter we use for croping the edges of the frame  \n",
        "        self.up=up\n",
        "        self.down=-down\n",
        "        self.left=left\n",
        "        self.right=-right\n",
        "    \n",
        "    def _preprocess_frame(self,frame):\n",
        "        \"\"\"\n",
        "        this function make preprocessing step for each frame from the game.\n",
        "        parameter: \n",
        "            -frame: is a row frame with [210*64*3] size which provided by the environment, after we make an action that change the world\n",
        "            the environmnt brovide us by new observation after apply that action.\n",
        "        return :\n",
        "            image: processing frame where we cut some of border and keep the play area and change the image to gray scale \n",
        "            then resize it to [84,84] shape and finaly normalize pixel value to become in the range [0-1] \n",
        "        \"\"\"\n",
        "        # cut the porder and keep the play area\n",
        "        image=frame[self.up:self.down,self.left:self.right,:]\n",
        "        image=tf.image.rgb_to_grayscale(image)\n",
        "        image=tf.image.resize(image,[84,84])\n",
        "        # reshape becouse it's tensor [84,84,1] and we want [84,84] for make the stacking operation easy\n",
        "        image=tf.reshape(image,[84,84])\n",
        "        # normalizing step\n",
        "        image=image/255.0\n",
        "        return image\n",
        "    \n",
        "    def ploting_function_one(frame):\n",
        "        \"\"\"\n",
        "        is the same as preprocessing function but we use it for viualization stuf.\n",
        "        \"\"\"\n",
        "        image=frame[25:,:,:]\n",
        "        image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "        image=cv2.resize(image,(85,85))\n",
        "        return image\n",
        "    \n",
        "    \n",
        "    def _state_creator(self,frame,is_new_episod):\n",
        "        \"\"\"\n",
        "        this function we stack last 4 frame togather to perform on state and that give us some intuition about vilocity.\n",
        "        parameter :\n",
        "            -frame :which is the game observation.\n",
        "            -is_new_episod: boolean parameter that check if there is no previous frame and that happend in bragning of each episod.\n",
        "        \n",
        "        return :\n",
        "            state: tensor with [84,84,4] shape which represent the state of our world.\n",
        "        \"\"\"\n",
        "        # first preprocessing the frame.\n",
        "        image=self._preprocess_frame(frame)\n",
        "        # if it's new episod then stack the first frame four time \n",
        "        if is_new_episod:\n",
        "            for i in range(self.stacked_size):\n",
        "                self.stacked_blocks.append(image)\n",
        "                \n",
        "        # just push the last frame so the first frame get out from the deque\n",
        "        else:\n",
        "            self.stacked_blocks.append(image)\n",
        "            \n",
        "        # stacked the frames togather in one tensor [84,84,4]\n",
        "        state=tf.stack(self.stacked_blocks,axis=2)\n",
        "        \n",
        "        return state\n",
        "    \n",
        "    def _sample_experiences(self):\n",
        "        \"\"\"\n",
        "        This function is use to sample batch from our memory.\n",
        "        \n",
        "        return:\n",
        "            -states:it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
        "            -actions: it's 2D array [batch_size,action] it's the action in each state in our batch\n",
        "            -rewards: it's 2D array [batch_size,reward] it's the reward in each state in out batch\n",
        "            -dones : it's 2D array [batch_size, done ] where done is boolean value help us to compute the target.\n",
        "            -next_state: it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
        "        \"\"\"\n",
        "#---->  instead of using random choice to select transition we use the prioritized memory object to sample data.\n",
        "        batch,weights,indices=self.replay_buffer.sample_experiences(self.batch_size)\n",
        "        # we combine the experiance togather where the buffer have tuple like this (state,action,reward,done,next_state)\n",
        "        states,actions,rewords,dones,next_states=[np.array([experiance[field_index] for experiance in batch]) for field_index in range(5)]\n",
        "#---->  we return the weights and inices becouse we need them for training step. \n",
        "        return states,actions,rewords,dones,next_states,weights,indices\n",
        "    \n",
        "    \n",
        "    def _epsilon_greedy_policy(self,state,epsilon):\n",
        "        \"\"\"\n",
        "        Is the epislon greedy policy whre we use epsilon value to chose an action\n",
        "        where we want to palance the exploration and explotation as possiable as we can.\n",
        "    \n",
        "        we pick random value alpha.\n",
        "            -if alpha < epsilon : chose random action\n",
        "             else argmax Q(state) for all action.\n",
        "        parameter :\n",
        "            -state: is the current state.\n",
        "            -epsilon : is the value of threshould between [0,1]\n",
        "        return :\n",
        "            the number of action to make \n",
        "    \n",
        "        \"\"\"\n",
        "        if np.random.rand()<epsilon:\n",
        "            return np.random.randint(self.action_space)\n",
        "        else:\n",
        "            # hear we use the online model for prediction of best action.\n",
        "            Q_values=self.online_model.predict(state[np.newaxis])\n",
        "            return tf.argmax(Q_values[0])\n",
        "    \n",
        "    def _play_one_step(self,state,epsilon):\n",
        "        \"\"\"\n",
        "        This function make an action and get the observation from the world.\n",
        "        we make an action and then get the full obesrvation from our environment aplay state_create function to the our \n",
        "        observation(frame) and then save it in the memory for training step.\n",
        "    \n",
        "        parameter :\n",
        "            -state : it's the current state.\n",
        "            -epsilon: it's the thresould we use for action selection\n",
        "        return :\n",
        "            next_state: tensore [84,84,4] block represent the last 4 frame.\n",
        "            reward : it's float number the reward we get after we do some action . \n",
        "            done : it's boolean refer if the game finish or not \n",
        "            info : it's dictionary have the counter of lives.\n",
        "        \"\"\"\n",
        "          \n",
        "        action=self._epsilon_greedy_policy(state,epsilon)\n",
        "        next_state,reward,done,info=self.env.step(action)\n",
        "        # we do that becouse we need True done value for computing the target for the model.\n",
        "        if info['ale.lives']< 5:\n",
        "            done=True\n",
        "        stacked_next_state=self._state_creator(next_state,False)\n",
        "#---->  hear we change the append function to the add function related to the Memory object         \n",
        "        self.replay_buffer.add((state,action,reward,done,stacked_next_state))\n",
        "        return stacked_next_state,reward,done,info\n",
        "    \n",
        "    def _training_step(self):\n",
        "        \"\"\"\n",
        "        This function is the responsible for doing one training step where we sample one batch from the replay buffer \n",
        "        then we push this batch in our model for forward step and apply gradiant with optimizer by hand.\n",
        "        \n",
        "        return :\n",
        "            None , where it's apply the change inplace for model parameter.\n",
        "        \"\"\"\n",
        "#---->  get the batch from the replay_buffer (our memory) with weights and indices.\n",
        "        states,actions,rewards,dones,next_states,weights,indices=self._sample_experiences()\n",
        "        # the online_Q(state,a)=online_Q(state,a) + alpha [ reward + gamma * target_Q(next_state,argmax online_Q(next_state,a_i) for all a)-online_Q(state,a)] \n",
        "        # as we can see hear we replace the prediction from the target model to online model as \"Hado van Hasselt 2015\" recomend \n",
        "        next_Q_values=self.online_model.predict(next_states)\n",
        "        # hear we take the best action over all action for the whole batch     \n",
        "        max_next_Q_values=np.argmax(next_Q_values,axis=1)\n",
        "        # create mask for reducing the value of actions that we don't used.        \n",
        "        next_mask = tf.one_hot(max_next_Q_values, self.action_space).numpy()\n",
        "\n",
        "        # compute the target which is [reward +  gamma * target_Q(next_state,argmx online_Q(next_state,a) over a) ]    \n",
        "        max_next_Q_values=(self.target_model.predict(next_states) * next_mask).sum(axis=1)\n",
        "        target_Q_values=rewards+(1-dones)*self.discount_factor*max_next_Q_values\n",
        "        \n",
        "        #this mask it's important for vanishing the value of actions that we don't used it's 2D array with 1 if action i was picked and zero othewise.\n",
        "        mask = tf.one_hot(actions, self.action_space)\n",
        "\n",
        "        #start the gradient recording to compute the gradient for our model.\n",
        "        with tf.GradientTape() as tape:\n",
        "            # compute the Q function for current state (hear for the whole batch).\n",
        "            all_Q_values=self.online_model(states)\n",
        "            #hear we use the mask so we reduce the result for just the action we picked.\n",
        "            Q_values=tf.reduce_sum(all_Q_values*mask,axis=1,keepdims=True)\n",
        "#----->     computer the loss function with the target we compute erlaier, these losses will be the new Priority .\n",
        "            losses=self.loss_function(target_Q_values,Q_values)\n",
        "#----->     computer the wighted loss then reduce to the mean so we can compute the gradeint             \n",
        "#----->     There is an Extenstion to this algorithm as the author of the paper suggest, that the new priority p_i=loss_i\n",
        "#----->     can be loss*weights so p_i=loss_i*weights_i then using the stochastic probability.\n",
        "#----->     it's so simple to modife the code for the extenstion but i will keep it as the peudocode in the paper.\n",
        "            loss = tf.reduce_mean(losses * weights)\n",
        "            \n",
        "        # compute the gradient for our  online_model parameter.\n",
        "        gradiants=tape.gradient(loss,self.online_model.trainable_variables)\n",
        "        # apply the optimization step using the gradient we compute for the online_model\n",
        "        self.optimizer.apply_gradients(zip(gradiants,self.online_model.trainable_variables))\n",
        "#---->  onther change where we update the priority of the transition we use in the training step.      \n",
        "        self.replay_buffer.update(indices,losses)\n",
        "        \n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        This function represent the training loop function where we are trying to fit model to produce the best function\n",
        "        according to the data we have.\n",
        "        \n",
        "        \"\"\"\n",
        "        for episode in tqdm.tqdm(range(self.episodes)):\n",
        "            state=self.env.reset()\n",
        "            stacked_state=self._state_creator(state,True)\n",
        "            rewards=0\n",
        "            epsilon=max(1-episode/5000,0.1)\n",
        "            # using this parameter to update the model parameter each 4 step .\n",
        "            minibatch_uppdate=0\n",
        "            while True:\n",
        "                stacked_state,reward,done,info=self._play_one_step(stacked_state,epsilon)\n",
        "                minibatch_uppdate += 1\n",
        "                rewards+=reward\n",
        "                if done :\n",
        "                    if not (episode%10) and len(self.rewards_list):\n",
        "                        self.online_model.save_weights(self.model_checkpoint)\n",
        "                        print(\"The best score for last 10 episode is: {} and the worst one is: {}\".format(max(self.rewards_list[-10:]),min(self.rewards_list[-10:])))\n",
        "                    self.rewards_list.append(rewards)\n",
        "                    break\n",
        "                # this step to let the replay buffer has some experiance\n",
        "                if episode>50:\n",
        "                    self._training_step()\n",
        "                #hear we update the target model after 50 episod by set it parameter as the online model.\n",
        "                if episode%self.update_rate:\n",
        "                    self.target_model.set_weights(self.online_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent=Agent(env,model)"
      ],
      "metadata": {
        "id": "dDUfWE5XVE1N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.fit()"
      ],
      "metadata": {
        "id": "q0OeGtqGVJP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kr4dly3xVLOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "DoubleDQN_with_ProportionalPrioritization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}