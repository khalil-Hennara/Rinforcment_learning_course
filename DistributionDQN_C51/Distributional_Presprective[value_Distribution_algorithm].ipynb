{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Wh-eYTTHd-i"
      },
      "outputs": [],
      "source": [
        "#import dependencies library\n",
        "# the enviornment library\n",
        "import gym\n",
        "\n",
        "#the AI framework \n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras \n",
        "import numpy as np\n",
        "\n",
        "#just for ploting stuf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install atari-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTk1tk_5IkvN",
        "outputId": "40212560-4807-43e9-b798-a4e783e47d35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS   rars\n",
        "!mv ROMS  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "metadata": {
        "id": "19hYGjPcHj6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU6lGp_qHd-w"
      },
      "source": [
        "After we import dependecies we will creat our enviornment using **gym** library and we chose the brakout game as the playground game to worke on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rd1mk7aJHd-1"
      },
      "outputs": [],
      "source": [
        "# make the environment we chose breakout game to try on.\n",
        "environment=gym.make(\"Breakout-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_b28QrMHd-3"
      },
      "source": [
        "lets explore the environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQlImnNlHd-4",
        "outputId": "cb6a6484-2c8d-4b74-b613-1d7e356ac506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
          ]
        }
      ],
      "source": [
        "print(environment.unwrapped.get_action_meanings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lefeLQgBHd-8",
        "outputId": "bc72aee8-ca10-48a6-dd6c-e970da888996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of action is :(4) and which is the number of output of Neural Network\n"
          ]
        }
      ],
      "source": [
        "# the actions space represent the output of the Neural Network \n",
        "action_space=environment.action_space.n\n",
        "print(\"The number of action is :({}) and which is the number of output of Neural Network\".format(action_space))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "xYvVdWayHd--",
        "outputId": "5c83c76b-5dc0-487b-b363-a29729fcf09d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f798f884410>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "state=environment.reset()\n",
        "plt.imshow(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6PCjZXHHd_A"
      },
      "source": [
        "The First step is make some preprocessing to each frame, where want change the **RGB** format to **gray** fromat and that decrease the confusing in the network and we will cut the top of frame which represent the reword which change and may confuse the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j7MVnOE5Hd_C"
      },
      "outputs": [],
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    This class is Warpper where we make sure that the game is runing, becouse in some environment in gym library should \n",
        "    start the game by action \" 1 \" in most of the game so by doing this we make sure that the game is roling.\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "        \n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "            obs, _, done, _ = self.env.step(2)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=FireResetEnv(environment)"
      ],
      "metadata": {
        "id": "rCAsIIV8Qyvr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u3LTCiChHd_c"
      },
      "outputs": [],
      "source": [
        "\n",
        "#The size of trainig example \n",
        "BATCH_SIZE=32\n",
        "\n",
        "# size of frame the input tensor to the model.\n",
        "WIDTH=84\n",
        "HIGHT=84\n",
        "CHANNELs=4\n",
        "\n",
        "# the Number of support in discrete distribution parameter \n",
        "ATOMS=51\n",
        "# the range that the support value derived from.\n",
        "VMIN=-10\n",
        "VMAX=10\n",
        "\n",
        "#numbe of action\n",
        "ACTIONS=4 #env.action_space.n\n",
        "\n",
        "# the step size between supports \n",
        "DELTA_Z=(VMAX-VMIN)/(ATOMS-1)\n",
        "\n",
        "# the distribuation parameter.\n",
        "Z=np.linspace(VMIN,VMAX,ATOMS,dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vGLKzkmgHd_f"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "class CategoricalDQN(keras.Model):\n",
        "    def __init__(self,output_dims,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        #The conv layers in the model \n",
        "        self.conv=keras.Sequential([\n",
        "            keras.layers.Conv2D(32,8,strides=4,padding='valid',activation='relu'),\n",
        "            keras.layers.Conv2D(64,4,strides=2,padding='valid',activation='relu'),\n",
        "            keras.layers.Conv2D(64,3,strides=1,padding='valid',activation='relu'),\n",
        "            keras.layers.Flatten()\n",
        "        ])\n",
        "        \n",
        "        self.fc=keras.layers.Dense(512,activation='relu')\n",
        "        self.fc_output=keras.layers.Dense(output_dims)\n",
        "        \n",
        "        \n",
        "    def call(self,x):\n",
        "        #find the batch this important if we try to pass one example or more so the batch is not fixed.\n",
        "        batch_size=x.shape[0]\n",
        "        conv_out = self.conv(x)\n",
        "        fc_1 = self.fc(conv_out)\n",
        "        # the output is [batch_size,Action*AToms] if actions is 4 the the shape is [batch_size,204]\n",
        "        out = self.fc_output(fc_1)\n",
        "        \n",
        "        # the reshape step were we reshape the output \n",
        "        out = tf.reshape(out,[batch_size,-1,ATOMS])\n",
        "        # apply the softmax function to the distribuation \n",
        "        return keras.activations.softmax(out,axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "384Ysu2VHd_h",
        "outputId": "93aa19b3-11f9-4e81-cc32-25fa12521126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"categorical_dqn_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_1 (Sequential)   (32, 3136)                77984     \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  1606144   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  104652    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,788,780\n",
            "Trainable params: 1,788,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model=CategoricalDQN(ACTIONS*ATOMS)\n",
        "model_2=CategoricalDQN(ACTIONS*ATOMS)\n",
        "#this step is just to print the summary report \n",
        "model.build([BATCH_SIZE,WIDTH,HIGHT,CHANNELs])\n",
        "model_2.build([BATCH_SIZE,WIDTH,HIGHT,CHANNELs])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4nd0qn0Hd_j",
        "outputId": "75cea910-d430-4fcf-b1d8-1c1757f4bbdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The state shape :(32, 84, 84, 4) , The rewards shape is :(32,) , The dones shapes is : (32,) ,The actions shape is (32,)\n"
          ]
        }
      ],
      "source": [
        "# we creat toy data for depuging the code it's good practice to do.\n",
        "state=np.random.rand(BATCH_SIZE,WIDTH,HIGHT,CHANNELs)\n",
        "next_states=np.random.rand(BATCH_SIZE,WIDTH,HIGHT,CHANNELs)\n",
        "rewards=np.random.rand(BATCH_SIZE)\n",
        "dones=np.random.randint(0,2,size=BATCH_SIZE)\n",
        "actions=np.random.randint(0,4,size=BATCH_SIZE)\n",
        "print(\"The state shape :{} , The rewards shape is :{} , The dones shapes is : {} ,The actions shape is {}\".format(state.shape,rewards.shape,dones.shape,actions.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "7Waj79ngHd_l"
      },
      "outputs": [],
      "source": [
        "# The optimizer is Adam as it's less sensetive to the learning rate value. the step size still 0.00025\n",
        "optimizer=keras.optimizers.Adam(learning_rate=0.00025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pKDKnDNpHd_n"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _epsilon_greedy_policy(state,epsilon):\n",
        "        \"\"\"\n",
        "        Is the epislon greedy policy whre we use epsilon value to chose an action\n",
        "        where we want to palance the exploration and explotation as possiable as we can.\n",
        "    \n",
        "        we pick random value alpha.\n",
        "            -if alpha < epsilon : chose random action\n",
        "             else argmax Q(state) for all action.\n",
        "        parameter :\n",
        "            -state: is the current state.\n",
        "            -epsilon : is the value of threshould between [0,1]\n",
        "        return :\n",
        "            the number of action to make \n",
        "    \n",
        "        \"\"\"\n",
        "        if np.random.rand()<epsilon:\n",
        "            return np.random.randint(ACTIONS)\n",
        "        else:\n",
        "            # hear we use the online model for prediction of best action.\n",
        "            # hears the model output is the Distribuation over each actions, so to get the best action\n",
        "            # we will get the mean over each distribuation then pick the best actions \n",
        "            propapility_dist=model(state[np.newaxis])\n",
        "            Q_values=tf.reduce_sum(propapility_dist*Z,axis=2)\n",
        "            return tf.argmax(Q_values[0]).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7h6_migoHd_o"
      },
      "outputs": [],
      "source": [
        "cross_loss=keras.losses.kl_divergence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMmJGN83Hd_p"
      },
      "outputs": [],
      "source": [
        "keras.losses.categorical_crossentropy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKcBxm_eHd_q"
      },
      "source": [
        "now the **Categorical** algorithm as in paper **Bellemare etc 2017** this function find the action-value distribuation as we discribe a bove blease read the comment for better understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "iAniL69JHd_r"
      },
      "outputs": [],
      "source": [
        "def distribution_projection(next_value_dist,rewards,gamma,dones):\n",
        "        batch_size=rewards.shape[0]\n",
        "        # in this line we calculate the next value return \n",
        "        T_Z=np.expand_dims(rewards,1)+ gamma*np.expand_dims((1-dones),1)*np.expand_dims(Z,0)\n",
        "        # clip the value in range [VMIN,VMAX]\n",
        "        clip_T_Z=np.clip(T_Z,VMIN,VMAX)\n",
        "        \n",
        "        # next return value postion  bj = (T_z - VMIN)/ Dz\n",
        "        value_dist_pos=(clip_T_Z-VMIN)/DELTA_Z\n",
        "        \n",
        "        # l= lower [bj] , u= upper [bj] \n",
        "        lower_bound=np.floor(value_dist_pos).astype(int)\n",
        "        upper_bound=np.ceil(value_dist_pos).astype(int)\n",
        "        \n",
        "        # this is the target distribuation  \n",
        "        target_distribuation = np.zeros((batch_size,ATOMS))\n",
        "        \n",
        "        for i in np.arange(batch_size):\n",
        "            for j in np.arange(ATOMS):\n",
        "                if lower_bound[i,j]==upper_bound[i,j]:\n",
        "                    target_distribuation[i,lower_bound[i,j]] += next_value_dist[i,j]\n",
        "                else:\n",
        "                    target_distribuation[i,lower_bound[i,j]]+=(next_value_dist[i,j]*(upper_bound-value_dist_pos)[i,j])\n",
        "                    target_distribuation[i,upper_bound[i,j]]+=(next_value_dist[i,j]*(value_dist_pos-lower_bound)[i,j])\n",
        "        return target_distribuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CWORe1rnHd_s"
      },
      "outputs": [],
      "source": [
        "def training_step(gamma):\n",
        "        \"\"\"\n",
        "        This function is the responsible for doing one training step where we sample one batch from the replay buffer \n",
        "        then we push this batch in our model for forward step and apply gradiant with optimizer by hand.\n",
        "        \n",
        "        return :\n",
        "            None , where it's apply the change inplace for model parameter.\n",
        "        \"\"\"\n",
        "#       get the transition (xt,at,rt,donet,xt+1) , gamma_t in [0,1] \n",
        "#         states,actions,rewards,dones,next_states=sample_experiences()\n",
        "#       we add comment in the sample experiences just for testing as we mentioned before so \n",
        "#       so this function when we put in out agent return as befor.\n",
        "#       we need the batch size\n",
        "        batch_size=state.shape[0]\n",
        "        \n",
        "        # find the propability of the next state the output shape is [batch,action,atoms] and hear we use model in the agent we will use target model \n",
        "        # hear we calculate p(xt+1,a) but for the whole BATCH \n",
        "        next_value_dist=model(next_states) \n",
        "        \n",
        "        # this is the Q(Xt+1,a) = Sum_i (zi*pi(xt+1,a)) the shape is [Batch,Actions] this is the Q_value for all actions \n",
        "        next_Q_values=tf.reduce_sum(next_value_dist*Z,axis=2) \n",
        "        \n",
        "        # a* = argmax Q(xt+1,a) over a   the shape is [Batch , 1]\n",
        "        best_next_actions=tf.argmax(next_Q_values,axis=1).numpy()\n",
        "        \n",
        "        # find the action-value distribuation for just the best action pj(xt+1,a*)\n",
        "        next_value_dist=tf.stack([ next_value_dist[index,best_next_actions[index]] for index in np.arange(batch_size)]) # batch,ATOMS \n",
        "        \n",
        "        target_distribuation = distribution_projection(next_value_dist,rewards,gamma,dones)\n",
        "#         print(tf.reduce_sum(next_value_dist,axis=1).numpy())\n",
        "#         print(target_distribuation.sum(axis=1))\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            # compute the Q function for current state (hear for the whole batch).\n",
        "            all_value_dist=model(state)\n",
        "            # as befor find the value distribuation for the action we made \n",
        "            action_value_dist=tf.stack([ all_value_dist[index,actions[index]] for index in np.arange(batch_size)])\n",
        "            #computer the log for the propability distribuation for finding the loss \n",
        "            # log Pi(xt,at)\n",
        "            log_action_value=tf.math.log(action_value_dist)\n",
        "            # compute the loss using KLD \n",
        "            loss= tf.reduce_sum(-log_action_value*target_distribuation)\n",
        "            print(loss)\n",
        "        # compute the gradient for our  online_model parameter.\n",
        "        gradiants=tape.gradient(loss,model.trainable_variables)\n",
        "        # apply the optimization step using the gradient we compute for the online_model\n",
        "        optimizer.apply_gradients(zip(gradiants,model.trainable_variables))\n",
        "        return (target_distribuation,action_value_dist)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target,pred=None,None\n",
        "target,pred=training_step(0.9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0KBVb9aKsV_",
        "outputId": "a1afc015-d81a-40ba-ec1f-c46fb86a713f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(125.67186, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot((target[15]),color='red',label='target')\n",
        "plt.plot((pred.numpy()[15]),color='green',label='pred')\n",
        "plt.legend()\n",
        "plt.savefig(\"First_move.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "t93eCpuBKxmr",
        "outputId": "aec95147-390f-48e5-fe1b-52e51cec5a95"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Z83kXJHd_u"
      },
      "outputs": [],
      "source": [
        "for i in range(200):\n",
        "    target,pred=training_step(0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OiY4Y2SaHd_w",
        "outputId": "0ca6d6c4-8425-463a-d46f-b823e8d372cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot((target[15]),color='red',label='target')\n",
        "plt.plot((pred.numpy()[15]),color='green',label='pred')\n",
        "plt.legend()\n",
        "plt.savefig(\"last_move.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTMoy3v-Hd_x",
        "outputId": "b17d1eb7-b682-4156-f2ab-ef713e8ae7e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.00048486, 0.00164331, 0.0007259 , 0.00109299, 0.00117237,\n",
              "       0.000491  , 0.00111796, 0.00300565, 0.00098123, 0.00197867,\n",
              "       0.00271171, 0.0016714 , 0.00290137, 0.00154731, 0.00216233,\n",
              "       0.00124292, 0.00182205, 0.00419685, 0.00187806, 0.00419077,\n",
              "       0.00251042, 0.00097648, 0.00387061, 0.0031114 , 0.00080369,\n",
              "       0.08834046, 0.17024238, 0.07587855, 0.11052132, 0.22123049,\n",
              "       0.11786267, 0.11456478, 0.01915019, 0.00205638, 0.00306321,\n",
              "       0.00259157, 0.00113591, 0.00168306, 0.00158307, 0.00153965,\n",
              "       0.00232866, 0.00150906, 0.00174798, 0.0051511 , 0.00175847,\n",
              "       0.00256628, 0.00083232, 0.00101314, 0.00119208, 0.0014637 ,\n",
              "       0.00070217], dtype=float32)"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "action_value_dist.numpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR3H6xV8HeAB"
      },
      "source": [
        "Now we will put this functin back in Agent class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "zu7QmqMjHeAC"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self,environment,model,model_2,discount_factor=0.99,learning_rate=0.00025,episode=10000,batch_size=32,replay_size=1000000,staked_frame=4,fixed_epsilon=None,model_checkpoint='DQN_wights.h5',up=25,down=10,left=5,right=5,update_steps=50):\n",
        "        #the environment simulator \n",
        "        self.env=environment\n",
        "        # hear we made the first change where we have 2 model instead of one model the online_model and target_model \n",
        "        #  the first one it's using for compute Q_value function at each time step while the other to just compute the target.\n",
        "        self.online_model=model\n",
        "        # the second model is the target model have the same structure of the online model and the same inial parameter.\n",
        "        self.target_model=model_2\n",
        "        self.target_model.set_weights(model.get_weights())\n",
        "        # the discout factor in the Q-learning algorithem\n",
        "        self.discount_factor=discount_factor\n",
        "        # the number of episod to run as the number of epochs in supervised learning \n",
        "        self.episodes=episode\n",
        "        # onather update to the previouse alogorithem where we need parameter that difine the update rate for the target model.\n",
        "        self.update_rate=update_steps\n",
        "    \n",
        "        self.batch_size=batch_size\n",
        "        # the memory where we save the observations for training \n",
        "        self.replay_buffer=deque(maxlen=replay_size)\n",
        "        # the deque used for state creation where we stacked last 4 frames togather \n",
        "        self.stacked_blocks=deque(maxlen=staked_frame)\n",
        "        # if we want to stack more or less frame togather in other world if we want the state more complex or simpler.\n",
        "        self.stacked_size=staked_frame\n",
        "        # if we want the epsilon parameter constant during the training else the parameter will change during the training.\n",
        "        self.fixed_epsilon=fixed_epsilon\n",
        "        # the action space it's represent the number neuron in the output layer becouse Q function is maping from state to actions. \n",
        "        self.action_space=environment.action_space.n\n",
        "        # it's rewards list where we save the reward from each episod ( from each epochs )\n",
        "        self.rewards_list=[]\n",
        "        # the loss function using to compute the gradiant of the model is KLD not MSE and we compute manualy.\n",
        "        # self.loss_function=tf.losses.mean_squared_error\n",
        "#---->   the optimizer using to update the parameter is no longer RMSprop.\n",
        "        self.optimizer=tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "        # it's the file path, where we save the model weights\n",
        "        self.model_checkpoint=model_checkpoint\n",
        "        # these four parameter we use for croping the edges of the frame  \n",
        "        self.up=up\n",
        "        self.down=-down\n",
        "        self.left=left\n",
        "        self.right=-right\n",
        "    \n",
        "    def _preprocess_frame(self,frame):\n",
        "        \"\"\"\n",
        "        this function make preprocessing step for each frame from the game.\n",
        "        parameter: \n",
        "            -frame: is a row frame with [210*64*3] size which provided by the environment, after we make an action that change the world\n",
        "            the environmnt brovide us by new observation after apply that action.\n",
        "        return :\n",
        "            image: processing frame where we cut some of border and keep the play area and change the image to gray scale \n",
        "            then resize it to [84,84] shape and finaly normalize pixel value to become in the range [0-1] \n",
        "        \"\"\"\n",
        "        # cut the porder and keep the play area\n",
        "        image=frame[self.up:self.down,self.left:self.right,:]\n",
        "        image=tf.image.rgb_to_grayscale(image)\n",
        "        image=tf.image.resize(image,[84,84])\n",
        "        # reshape becouse it's tensor [84,84,1] and we want [84,84] for make the stacking operation easy\n",
        "        image=tf.reshape(image,[84,84])\n",
        "        # normalizing step\n",
        "        image=image/255.0\n",
        "        return image\n",
        "    \n",
        "    def ploting_function_one(frame):\n",
        "        \"\"\"\n",
        "        is the same as preprocessing function but we use it for viualization stuf.\n",
        "        \"\"\"\n",
        "        image=frame[25:,:,:]\n",
        "        image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
        "        image=cv2.resize(image,(85,85))\n",
        "        return image\n",
        "    \n",
        "    \n",
        "    def _state_creator(self,frame,is_new_episod):\n",
        "        \"\"\"\n",
        "        this function we stack last 4 frame togather to perform on state and that give us some intuition about vilocity.\n",
        "        parameter :\n",
        "            -frame :which is the game observation.\n",
        "            -is_new_episod: boolean parameter that check if there is no previous frame and that happend in bragning of each episod.\n",
        "        \n",
        "        return :\n",
        "            state: tensor with [84,84,4] shape which represent the state of our world.\n",
        "        \"\"\"\n",
        "        # first preprocessing the frame.\n",
        "        image=self._preprocess_frame(frame)\n",
        "        # if it's new episod then stack the first frame four time \n",
        "        if is_new_episod:\n",
        "            for i in range(self.stacked_size):\n",
        "                self.stacked_blocks.append(image)\n",
        "                \n",
        "        # just push the last frame so the first frame get out from the deque\n",
        "        else:\n",
        "            self.stacked_blocks.append(image)\n",
        "            \n",
        "        # stacked the frames togather in one tensor [84,84,4]\n",
        "        state=tf.stack(self.stacked_blocks,axis=2)\n",
        "        \n",
        "        return state\n",
        "    \n",
        "    def _sample_experiences(self):\n",
        "        \"\"\"\n",
        "        This function is use to sample batch from our memory.\n",
        "        \n",
        "        return:\n",
        "            -states:it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
        "            -actions: it's 2D array [batch_size,action] it's the action in each state in our batch\n",
        "            -rewards: it's 2D array [batch_size,reward] it's the reward in each state in out batch\n",
        "            -dones : it's 2D array [batch_size, done ] where done is boolean value help us to compute the target.\n",
        "            -next_state: it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
        "        \"\"\"\n",
        "        # radnom indices from the replay buffer\n",
        "        indices=np.random.randint(len(self.replay_buffer),size=self.batch_size)\n",
        "        batch=[self.replay_buffer[index] for index in indices]\n",
        "        # we combine the experiance togather where the buffer have tuple like this (state,action,reward,done,next_state)\n",
        "        states,actions,rewords,dones,next_states=[np.array([experiance[field_index] for experiance in batch]) for field_index in range(5)]\n",
        "        return states,actions,rewords,dones,next_states\n",
        "    \n",
        "    \n",
        "    \n",
        "    def _epsilon_greedy_policy(self,state,epsilon):\n",
        "        \"\"\"\n",
        "        Is the epislon greedy policy whre we use epsilon value to chose an action\n",
        "        where we want to palance the exploration and explotation as possiable as we can.\n",
        "    \n",
        "        we pick random value alpha.\n",
        "            -if alpha < epsilon : chose random action\n",
        "             else argmax Q(state) for all action.\n",
        "        parameter :\n",
        "            -state: is the current state.\n",
        "            -epsilon : is the value of threshould between [0,1]\n",
        "        return :\n",
        "            the number of action to make \n",
        "    \n",
        "        \"\"\"\n",
        "        if np.random.rand()<epsilon:\n",
        "            return np.random.randint(self.action_space)\n",
        "        else:\n",
        "            # hear we use the online model for prediction of best action.\n",
        "            # hears the model output is the Distribuation over each actions, so to get the best action\n",
        "            # we will get the mean over each distribuation then pick the best actions \n",
        "            propapility_dist=self.online_model(state[np.newaxis])\n",
        "            Q_values=tf.reduce_sum(propapility_dist*Z,axis=2)\n",
        "            return tf.argmax(Q_values[0]).numpy()\n",
        "    \n",
        "    def _play_one_step(self,state,epsilon):      \n",
        "      action=self._epsilon_greedy_policy(state,epsilon)\n",
        "      next_state,reward,done,info=self.env.step(action)\n",
        "      # we do that becouse we need True done value for computing the target for the model.\n",
        "      if info['ale.lives']< 5:\n",
        "          done=True\n",
        "      stacked_next_state=self._state_creator(next_state,False)\n",
        "      self.replay_buffer.append((state,action,reward,done,stacked_next_state))\n",
        "      return stacked_next_state,reward,done,info\n",
        "    \n",
        "    def _training_step(self):\n",
        "        \"\"\"\n",
        "        This function is the responsible for doing one training step where we sample one batch from the replay buffer \n",
        "        then we push this batch in our model for forward step and apply gradiant with optimizer by hand.\n",
        "        \n",
        "        return :\n",
        "            None , where it's apply the change inplace for model parameter.\n",
        "        \"\"\"\n",
        "        # get the transition (xt,at,rt,donet,xt+1) , gamma_t in [0,1] \n",
        "        states,actions,rewards,dones,next_states=self._sample_experiences()\n",
        "        # we need the batch size\n",
        "        batch_size=state.shape[0]\n",
        "        # find the propability of the next state the output shape is [batch,action,atoms] and hear we use model in the agent we will use target model \n",
        "        # hear we calculate p(xt+1,a) but for the whole BATCH \n",
        "        next_value_dist=self.target_model(next_states) \n",
        "        # this is the Q(Xt+1,a) = Sum_i (zi*pi(xt+1,a)) the shape is [Batch,Actions] this is the Q_value for all actions \n",
        "        next_Q_values=tf.reduce_sum(next_value_dist*Z,axis=2) \n",
        "        # a* = argmax Q(xt+1,a) over a   the shape is [Batch , 1]\n",
        "        best_next_actions=tf.argmax(next_Q_values,axis=1).numpy()\n",
        "        # find the action-value distribuation for just the best action pj(xt+1,a*)\n",
        "        next_value_dist=tf.stack([ next_value_dist[index,best_next_actions[index]] for index in np.arange(batch_size)]) # batch,ATOMS \n",
        "        # in this line we calculate the next value return [rt + gamma_t * zj ]\n",
        "        T_Z=np.expand_dims(rewards,1)+ self.discount_factor * np.expand_dims((1-dones),1)*np.expand_dims(Z,0)\n",
        "        # clip the value in range [VMIN,VMAX]\n",
        "        clip_T_Z=np.clip(T_Z,VMIN,VMAX)\n",
        "        # next return value postion  bj = (T_z - VMIN)/ Dz\n",
        "        value_dist_pos=(clip_T_Z-VMIN)/DELTA_Z\n",
        "        \n",
        "        # l= lower [bj] , u= upper [bj] \n",
        "        lower_bound=np.floor(value_dist_pos).astype(int)\n",
        "        upper_bound=np.ceil(value_dist_pos).astype(int)\n",
        "        \n",
        "        # this is the target distribuation  \n",
        "        target_distribuation = np.zeros((batch_size,ATOMS))\n",
        "        \n",
        "        for i in np.arange(batch_size):\n",
        "            for j in np.arange(ATOMS):\n",
        "                target_distribuation[i,lower_bound[i,j]]+=(next_value_dist[i,j]*(value_dist_pos-lower_bound)[i,j])\n",
        "                target_distribuation[i,upper_bound[i,j]]+=(next_value_dist[i,j]*(upper_bound-value_dist_pos)[i,j])\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            # compute the Q function for current state (hear for the whole batch).\n",
        "            all_value_dist=self.online_model(state)\n",
        "            # as befor find the value distribuation for the action we made \n",
        "            action_value_dist=tf.stack([ all_value_dist[index,actions[index]] for index in np.arange(batch_size)])\n",
        "            #computer the log for the propability distribuation for finding the loss \n",
        "            # log Pi(xt,at)\n",
        "            log_action_value=tf.math.log(action_value_dist)\n",
        "            # compute the loss using KLD \n",
        "            loss= tf.reduce_sum(-log_action_value*target_distribuation)\n",
        "        # compute the gradient for our  online_model parameter.\n",
        "        gradiants=tape.gradient(loss,model.trainable_variables)\n",
        "        # apply the optimization step using the gradient we compute for the online_model\n",
        "        optimizer.apply_gradients(zip(gradiants,model.trainable_variables))\n",
        "        \n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        This function represent the training loop function where we are trying to fit model to produce the best function\n",
        "        according to the data we have.\n",
        "        \n",
        "        \"\"\"\n",
        "        for episode in tqdm.tqdm(range(self.episodes)):\n",
        "            state=self.env.reset()\n",
        "            stacked_state=self._state_creator(state,True)\n",
        "            rewards=0\n",
        "            epsilon=max(1-episode/7000,0.01)\n",
        "            while True:\n",
        "                stacked_state,reward,done,info=self._play_one_step(stacked_state,epsilon)\n",
        "                rewards+=reward\n",
        "                if done :\n",
        "                    if not (episode%10) and len(self.rewards_list):\n",
        "                        self.online_model.save_weights(self.model_checkpoint)\n",
        "                        print(\"The best score for last 10 episode is: {} and the worst one is: {}\".format(max(self.rewards_list[-10:]),min(self.rewards_list[-10:])))\n",
        "                    self.rewards_list.append(rewards)\n",
        "                    break\n",
        "                # this step to let the replay buffer has some experiance\n",
        "                if episode>50:\n",
        "                    self._training_step()\n",
        "                #hear where update the target model after 50 episod by set the parameter as online model.\n",
        "                if episode%self.update_rate:\n",
        "                    self.target_model.set_weights(self.online_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(env,model,model_2)"
      ],
      "metadata": {
        "id": "xGtCPND6P0KI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.fit()"
      ],
      "metadata": {
        "id": "lH8fZcwfQi6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TBWnJboVRgHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "Distributional_Presprective[value_Distribution_algorithm].ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}