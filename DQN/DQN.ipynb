{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies library\n",
    "\n",
    "# the enviornment library\n",
    "import gym\n",
    "\n",
    "#the AI framework \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras \n",
    "import numpy as np\n",
    "\n",
    "#just for ploting stuf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we import dependecies we will creat our enviornment using **gym** library and we chose the brakout game as the playground game to worke on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the environment we chose breakout game to try on.\n",
    "environment=gym.make(\"Breakout-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets explore the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "print(environment.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of action is :(4) and which is the number of output of Neural Network\n"
     ]
    }
   ],
   "source": [
    "# the actions space represent the output of the Neural Network \n",
    "action_space=environment.action_space.n\n",
    "print(\"The number of action is :({}) and which is the number of output of Neural Network\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9d471c3940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state=environment.reset()\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The First step is make some preprocessing to each frame, where want change the **RGB** format to **gray** fromat and that decrease the confusing in the network and we will cut the top of frame which represent the reword which change and may confuse the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    this function make preprocessing step for each frame from the game.\n",
    "    parameter: \n",
    "        -frame: is a row frame with [210*64*3] size which provided by the environment, after we make an action that change the world\n",
    "        the environmnt brovide us by new observation after apply that action.\n",
    "    return :\n",
    "        image: processing frame where we cut some of border and keep the play area and change the image to gray scale \n",
    "        then resize it to [84,84] shape and finaly normalize pixel value to become in the range [0-1] \n",
    "    \"\"\"\n",
    "    image=frame[25:-10,5:-5,:]\n",
    "    image=tf.image.rgb_to_grayscale(image)\n",
    "    image=tf.image.resize(image,[84,84])\n",
    "    image=tf.reshape(image,[84,84])\n",
    "    image=image/255.0\n",
    "    return image\n",
    "\n",
    "def ploting_function_one(frame):\n",
    "    \"\"\"\n",
    "    is the same as preprocessing function but we use it for viualization stuf.\n",
    "    \"\"\"\n",
    "    image=frame[25:,:,:]\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    image=cv2.resize(image,(84,84))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    This class is Warpper where we make sure that the game is runing, becouse in some environment in gym library should \n",
    "    start the game by action \" 1 \" in most of the game so by doing this we make sure that the game is roling.\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "            obs, _, done, _ = self.env.step(2)\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment=FireResetEnv(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell containe two preprocessing function one for visulisation becoues we can't plot tensor from tesorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f36b871ca20>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image=ploting_function_one(state)\n",
    "plt.imshow(image,cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now after we processing the frame we need to stack some frames togather to represent a one state so by using that we can give some information about vilocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first leats creat queue with fixed size 4 so after each move we push the frame to the queue and the stack them togather to represent on 3D block the represent the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_blocks=deque(maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_creator(frame,is_new_episod,stacked_blocks,number_of_frame=4):\n",
    "    \"\"\"\n",
    "    in this function we stack last 4 frame togather to perform on state and that give us some intuition about vilocity.\n",
    "    parameter :\n",
    "        -frame :which is the game observation.\n",
    "        -is_new_episod: boolean parameter that check if there is no previous frame and that happend in bragning of each episod.\n",
    "        -stacked_blocks: is deque with maxlength 4 that save the last 4 frame for us.\n",
    "        -number_of_frame: is by default 4 but we can change it to other number and that make our state more complex.\n",
    "    return :\n",
    "        state: tensor with [84,84,4] shape which represent the state of our world.\n",
    "    \"\"\"\n",
    "    if is_new_episod:\n",
    "        image=preprocess_frame(frame)\n",
    "        for i in range(number_of_frame):\n",
    "            stacked_blocks.append(image)\n",
    "    else:\n",
    "        image=preprocess_frame(frame)\n",
    "        stacked_blocks.append(image)\n",
    "    state=tf.stack(stacked_blocks,axis=2)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the frames which is stack togather is : (85, 85, 4)\n"
     ]
    }
   ],
   "source": [
    "new_state=state_creator(state,True,stacked_blocks)\n",
    "print(\"The shape of the frames which is stack togather is : {}\".format(new_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build the model which is convulational Neural Network that contains just 2 layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 43, 43, 16)        4112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 21, 21, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 21, 21, 32)        131104    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 963,028\n",
      "Trainable params: 963,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#this model is as recomende from paper \"Mnih 2013, Playing Atari with Deep Reinforcement Learning\"\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model=keras.Sequential([\n",
    "    keras.layers.Conv2D(16,8,strides=4,padding='valid',activation='relu',input_shape=[84,84,4]),\n",
    "    keras.layers.Conv2D(32,4,strides=2,padding='valid',activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256,activation='relu'),\n",
    "    keras.layers.Dense(action_space)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will use replay puffer to collect observation and return batch for training from the buffer by that we reducing the noise and make the learning process much faster using **GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this represent the memory where we keep the last million state for training.\n",
    "replay_buffer=deque(maxlen=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    \"\"\"\n",
    "    This function is use to sample batch from our memory.\n",
    "    parameter : \n",
    "            -batch size: is numeber of example that provided to our network. \n",
    "    return:\n",
    "            -states:it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
    "            -actions: it's 2D array [batch_size,action] it's the action in each state in our batch\n",
    "            -rewards: it's 2D array [batch_size,reward] it's the reward in each state in out batch\n",
    "            -dones : it's 2D array [batch_size, done ] where done is boolean value help us to compute the target.\n",
    "            -next_state: it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
    "    \"\"\"\n",
    "    indices=np.random.randint(len(replay_buffer),size=batch_size)\n",
    "    batch=[replay_buffer[index] for index in indices]\n",
    "    states,actions,rewords,dones,next_states=[np.array([experiance[field_index] for experiance in batch]) for field_index in range(5)]\n",
    "    return states,actions,rewords,dones,next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,epsilon):\n",
    "    \"\"\"\n",
    "    Is the epislon greedy policy whre we use epsilon value to chose an action\n",
    "    where we want to palance the exploration and explotation as possiable as we can.\n",
    "    \n",
    "    we pick random value alpha.\n",
    "        -if alpha < epsilon : chose random action\n",
    "         else argmax Q(state) for all action.\n",
    "    parameter :\n",
    "        -state: is the current state.\n",
    "        -epsilon : is the value of threshould between [0,1]\n",
    "    return :\n",
    "        the number of action to make \n",
    "    \n",
    "    \"\"\"\n",
    "    if np.random.rand()<epsilon:\n",
    "        return np.random.randint(action_space)\n",
    "    else:\n",
    "        Q_values=model.predict(state[np.newaxis])\n",
    "        return tf.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env,state,epsilon):\n",
    "    \"\"\"\n",
    "    This function make an action and get the observation from the world.\n",
    "    we make an action and then get the full obesrvation from our environment aplay state_create function to the our \n",
    "    observation(frame) and then save it in the memory for training step.\n",
    "    \n",
    "    parameter :\n",
    "        -env: it's represent our world or environment we work on \n",
    "        -state : it's the current state.\n",
    "        -epsilon: it's the thresould we use for action selection\n",
    "    return :\n",
    "        next_state: tensore [84,84,4] block represent the last 4 frame.\n",
    "        reward : it's float number the reward we get after we do some action . \n",
    "        done : it's boolean refer if the game finish or not \n",
    "        info : it's dictionary have the counter of lives.\n",
    "    \"\"\"\n",
    "    action=epsilon_greedy_policy(state,epsilon)\n",
    "    next_state,reward,done,info=env.step(action)\n",
    "    if info['ale.lives']< 5:\n",
    "            done=True\n",
    "    stacked_next_state=state_creator(next_state,False,stacked_blocks)\n",
    "    replay_buffer.append((state,action,reward,done,stacked_next_state))\n",
    "    return stacked_next_state,reward,done,info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=tf.losses.mean_squared_error\n",
    "optimizer=tf.optimizers.Adam(lr=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size,discount_factor):\n",
    "    \"\"\"\n",
    "    This function is the responsible for doing one training step where we sample one batch from the replay buffer \n",
    "    then we push this batch in our model for forward step and apply gradiant with optimizer by hand.\n",
    "    parameter :\n",
    "        -batch_size : the number of training example we sample\n",
    "        dicout_factor: this parameter is responsible for future rewards important it's value must be in range[0-1]\n",
    "    return :\n",
    "        None , where it's apply the change inplace for model parameter.\n",
    "    \"\"\"\n",
    "    states,actions,rewards,dones,next_states=sample_experiences(batch_size)\n",
    "    next_Q_values=model(next_states)\n",
    "    max_next_Q_values=np.max(next_Q_values,axis=1)\n",
    "    target_Q_values=rewards+(1-dones)*discount_factor*max_next_Q_values\n",
    "    \n",
    "    mask = tf.one_hot(actions, action_space)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values=model(states)\n",
    "        Q_values=tf.reduce_sum(all_Q_values*mask,axis=1,keepdims=True)\n",
    "        loss=tf.reduce_mean(loss_function(target_Q_values,Q_values))\n",
    "    gradiants=tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradiants,model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=700\n",
    "epsilon=1.0\n",
    "batch_size=5\n",
    "discount_factor=0.98\n",
    "\n",
    "for episode in tqdm.tqdm(range(episodes)):\n",
    "    state=environment.reset()\n",
    "    stacked_state=state_creator(state,True,stacked_blocks)\n",
    "    rewards=0\n",
    "    epsilon=max(1-episode/500,0.1)\n",
    "    while True:\n",
    "        stacked_state,reward,done=play_one_step(environment,stacked_state,epsilon)\n",
    "        rewards+=reward\n",
    "        if done:\n",
    "            if not (episode%10) :\n",
    "                print(rewards)\n",
    "            break\n",
    "            \n",
    "        if len(replay_buffer)>10:\n",
    "            training_step(batch_size,discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets put all togather in one class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self,environment,model,discount_factor=0.95,learning_rate=0.00025,episode=1000,batch_size=32,replay_size=1000000,staked_frame=4,fixed_epsilon=None,model_checkpoint='DQN_wights.h5',up=25,down=10,left=5,right=5):\n",
    "        #the environment simulator \n",
    "        self.env=environment\n",
    "        # the model we don't build the model and that give the developer space so he can change the structure of the model.\n",
    "        self.model=model\n",
    "        # the discout factor in the Q-learning algorithem\n",
    "        self.discount_factor=discount_factor\n",
    "        # the number of episod to run as the number of epochs in supervised learning \n",
    "        self.episodes=episode\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        # the memory where we save the observations for training \n",
    "        self.replay_buffer=deque(maxlen=replay_size)\n",
    "        # the deque used for state creation where we stacked last 4 frames togather \n",
    "        self.stacked_blocks=deque(maxlen=staked_frame)\n",
    "        # if we want to stack more or less frame togather in other world if we want the state more complex or simpler.\n",
    "        self.stacked_size=staked_frame\n",
    "        # if we want the epsilon parameter constant during the training else the parameter will change during the training.\n",
    "        self.fixed_epsilon=fixed_epsilon\n",
    "        # the action space it's represent the number neuron in the output layer becouse Q function is maping from state to actions. \n",
    "        self.action_space=environment.action_space.n\n",
    "        # it's rewards list where we save the reward from each episod ( from each epochs )\n",
    "        self.rewards_list=[]\n",
    "        # the loss function using to compute the gradiant of the model.\n",
    "        self.loss_function=tf.losses.mean_squared_error\n",
    "        # the optimizer using to update the parameter.\n",
    "        self.optimizer=tf.optimizers.RMSprop(learning_rate=learning_rate,momentum=0.95)\n",
    "        # it's the file path, where we save the model weights\n",
    "        self.model_checkpoint=model_checkpoint\n",
    "        # these four parameter we use for croping the edges of the frame  \n",
    "        self.up=up\n",
    "        self.down=-down\n",
    "        self.left=left\n",
    "        self.right=-right\n",
    "        \n",
    "    \n",
    "    def _preprocess_frame(self,frame):\n",
    "        \"\"\"\n",
    "        this function make preprocessing step for each frame from the game.\n",
    "        parameter: \n",
    "            -frame: is a row frame with [210*64*3] size which provided by the environment, after we make an action that change the world\n",
    "            the environmnt brovide us by new observation after apply that action.\n",
    "        return :\n",
    "            image: processing frame where we cut some of border and keep the play area and change the image to gray scale \n",
    "            then resize it to [84,84] shape and finaly normalize pixel value to become in the range [0-1] \n",
    "        \"\"\"\n",
    "        # cut the porder and keep the play area\n",
    "        image=frame[self.up:self.down,self.left:self.right,:]\n",
    "        image=tf.image.rgb_to_grayscale(image)\n",
    "        image=tf.image.resize(image,[84,84])\n",
    "        # reshape becouse it's tensor [84,84,1] and we want [84,84] for make the stacking operation easy\n",
    "        image=tf.reshape(image,[84,84])\n",
    "        # normalizing step\n",
    "        image=image/255.0\n",
    "        return image\n",
    "    \n",
    "    def ploting_function_one(frame):\n",
    "        \"\"\"\n",
    "        is the same as preprocessing function but we use it for viualization stuf.\n",
    "        \"\"\"\n",
    "        image=frame[25:,:,:]\n",
    "        image=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "        image=cv2.resize(image,(85,85))\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    def _state_creator(self,frame,is_new_episod):\n",
    "        \"\"\"\n",
    "        this function we stack last 4 frame togather to perform on state and that give us some intuition about vilocity.\n",
    "        parameter :\n",
    "            -frame :which is the game observation.\n",
    "            -is_new_episod: boolean parameter that check if there is no previous frame and that happend in bragning of each episod.\n",
    "        \n",
    "        return :\n",
    "            state: tensor with [84,84,4] shape which represent the state of our world.\n",
    "        \"\"\"\n",
    "        # first preprocessing the frame.\n",
    "        image=self._preprocess_frame(frame)\n",
    "        # if it's new episod then stack the first frame four time \n",
    "        if is_new_episod:\n",
    "            for i in range(self.stacked_size):\n",
    "                self.stacked_blocks.append(image)\n",
    "                \n",
    "        # just push the last frame so the first frame get out from the deque\n",
    "        else:\n",
    "            self.stacked_blocks.append(image)\n",
    "            \n",
    "        # stacked the frames togather in one tensor [84,84,4]\n",
    "        state=tf.stack(self.stacked_blocks,axis=2)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _sample_experiences(self):\n",
    "        \"\"\"\n",
    "        This function is use to sample batch from our memory.\n",
    "        \n",
    "        return:\n",
    "            -states:it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
    "            -actions: it's 2D array [batch_size,action] it's the action in each state in our batch\n",
    "            -rewards: it's 2D array [batch_size,reward] it's the reward in each state in out batch\n",
    "            -dones : it's 2D array [batch_size, done ] where done is boolean value help us to compute the target.\n",
    "            -next_state: it's tensore [batch_size,image_width,image_height,stacked_size] in our case [32,84,84,4]\n",
    "        \"\"\"\n",
    "        # radnom indices from the replay buffer\n",
    "        indices=np.random.randint(len(self.replay_buffer),size=self.batch_size)\n",
    "        batch=[self.replay_buffer[index] for index in indices]\n",
    "        # we combine the experiance togather where the buffer have tuple like this (state,action,reward,done,next_state)\n",
    "        states,actions,rewords,dones,next_states=[np.array([experiance[field_index] for experiance in batch]) for field_index in range(5)]\n",
    "        return states,actions,rewords,dones,next_states\n",
    "    \n",
    "    \n",
    "    def _epsilon_greedy_policy(self,state,epsilon):\n",
    "        \"\"\"\n",
    "        Is the epislon greedy policy whre we use epsilon value to chose an action\n",
    "        where we want to palance the exploration and explotation as possiable as we can.\n",
    "    \n",
    "        we pick random value alpha.\n",
    "            -if alpha < epsilon : chose random action\n",
    "             else argmax Q(state) for all action.\n",
    "        parameter :\n",
    "            -state: is the current state.\n",
    "            -epsilon : is the value of threshould between [0,1]\n",
    "        return :\n",
    "            the number of action to make \n",
    "    \n",
    "        \"\"\"\n",
    "        if np.random.rand()<epsilon:\n",
    "            return np.random.randint(self.action_space)\n",
    "        else:\n",
    "            Q_values=self.model.predict(state[np.newaxis])\n",
    "            return tf.argmax(Q_values[0])\n",
    "    \n",
    "    def _play_one_step(self,state,epsilon):\n",
    "         \"\"\"\n",
    "        This function make an action and get the observation from the world.\n",
    "        we make an action and then get the full obesrvation from our environment aplay state_create function to the our \n",
    "        observation(frame) and then save it in the memory for training step.\n",
    "    \n",
    "        parameter :\n",
    "            -state : it's the current state.\n",
    "            -epsilon: it's the thresould we use for action selection\n",
    "        return :\n",
    "            next_state: tensore [84,84,4] block represent the last 4 frame.\n",
    "            reward : it's float number the reward we get after we do some action . \n",
    "            done : it's boolean refer if the game finish or not \n",
    "            info : it's dictionary have the counter of lives.\n",
    "        \"\"\"\n",
    "          \n",
    "        action=self._epsilon_greedy_policy(state,epsilon)\n",
    "        next_state,reward,done,info=self.env.step(action)\n",
    "        # we do that becouse we need True done value for computing the target for the model.\n",
    "        if info['ale.lives']< 5:\n",
    "            done=True\n",
    "        stacked_next_state=self._state_creator(next_state,False)\n",
    "        self.replay_buffer.append((state,action,reward,done,stacked_next_state))\n",
    "        return stacked_next_state,reward,done,info\n",
    "    \n",
    "    def _training_step(self):\n",
    "        \"\"\"\n",
    "        This function is the responsible for doing one training step where we sample one batch from the replay buffer \n",
    "        then we push this batch in our model for forward step and apply gradiant with optimizer by hand.\n",
    "        \n",
    "        return :\n",
    "            None , where it's apply the change inplace for model parameter.\n",
    "        \"\"\"\n",
    "        # get the batch from the replay_buffer (our memory)\n",
    "        states,actions,rewards,dones,next_states=self._sample_experiences()\n",
    "        #the Q(state,a)=Q(state,a) + alpha [ reward + gamma * max (Q(next_state,a) for all a)-Q(s,a)] \n",
    "        next_Q_values=self.model.predict(next_states)\n",
    "        max_next_Q_values=np.max(next_Q_values,axis=1)\n",
    "        # compute the target which is [reward +  gamma * max(Q(next_state,a) over a)] \n",
    "        # (1-dones) hear refer for the last move (final state) in trajectory so it's should be just the immediat reward becouse there is no next state\n",
    "        target_Q_values=rewards+(1-dones)*self.discount_factor*max_next_Q_values\n",
    "        \n",
    "        #this mask it's important for vanishing the value of actions that we don't used it's 2D array with 1 if action i was picked and zero othewise.\n",
    "        mask = tf.one_hot(actions, self.action_space)\n",
    "\n",
    "        #start the gradient recording to compute the gradient for our model.\n",
    "        with tf.GradientTape() as tape:\n",
    "            #compute the Q function for state (hear for the whole batch).\n",
    "            all_Q_values=self.model(states)\n",
    "            #hear we use the mask so we reduce the result for just the action we picked.\n",
    "            Q_values=tf.reduce_sum(all_Q_values*mask,axis=1,keepdims=True)\n",
    "            #computer the loss function with the target we compute erlaier.\n",
    "            loss=tf.reduce_mean(self.loss_function(target_Q_values,Q_values))\n",
    "        # compute the gradient for our model parameter.\n",
    "        gradiants=tape.gradient(loss,self.model.trainable_variables)\n",
    "        # apply the optimization step using the gradient we compute \n",
    "        self.optimizer.apply_gradients(zip(gradiants,self.model.trainable_variables))\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        This function represent the training loop function where we are trying to fit model to produce the best function\n",
    "        according to the data we have.\n",
    "        \n",
    "        \"\"\"\n",
    "        for episode in tqdm.tqdm(range(self.episodes)):\n",
    "            state=self.env.reset()\n",
    "            stacked_state=self._state_creator(state,True)\n",
    "            rewards=0\n",
    "            epsilon=max(1-episode/600,0.1)\n",
    "            while True:\n",
    "                stacked_state,reward,done,info=self._play_one_step(stacked_state,epsilon)\n",
    "                rewards+=reward\n",
    "                if done :\n",
    "                    if not (episode%10) and len(self.rewards_list):\n",
    "                        self.model.save_weights(self.model_checkpoint)\n",
    "                        print(\"The best score for last 10 episode is: {} and the worst one is: {}\".format(max(self.rewards_list[-10:]),min(self.rewards_list[-10:])))\n",
    "                    self.rewards_list.append(rewards)\n",
    "                    break\n",
    "                # this step to let the replay buffer has some experiance\n",
    "                if episode>50:\n",
    "                    self._training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn=DQN_Agent(environment,model)\n",
    "dqn.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
