{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb3383e5128>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_frames=deque(maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame=frame[30:,:,:] #we cut the top of the frame to a void Misleading decision of the model.\n",
    "    frame=cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
    "    frame=frame/255.0\n",
    "    return frame\n",
    "\n",
    "def stack_frames(frame,done):\n",
    "    \"\"\"\n",
    "    this stacks each 4 frame togother to recognise the movement in the frame\n",
    "    this is technique use in alot of deep learning problem.\n",
    "    \n",
    "    \"\"\"\n",
    "    frame=preprocess_frame(frame)  \n",
    "    if done:\n",
    "        for i in range(4):\n",
    "            stacked_frames.append(frame)\n",
    "    stacked_frames.append(frame)\n",
    "    input_stacks=np.stack(stacked_frames,axis=2)\n",
    "    return input_stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=[180,160,4]\n",
    "output_shape= env.action_space.n\n",
    "epochs=1000\n",
    "n_steps=200\n",
    "batch_size=64\n",
    "buffer_size=10000\n",
    "gamma=0.95\n",
    "epsilon=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 180, 160, 32)      28832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 90, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 90, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 90, 80, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 45, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 45, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 45, 40, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56320)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1802272   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 1,956,356\n",
      "Trainable params: 1,956,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "model=keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32,15,activation='elu',padding='same',input_shape=input_shape),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Conv2D(64,5,activation='elu',padding='same'),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Conv2D(128,3,activation='elu',padding='same'),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32,activation='relu'),\n",
    "    keras.layers.Dense(output_shape)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer=deque(maxlen=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,epsilon):\n",
    "    if np.random.rand()<epsilon:\n",
    "        return np.random.randint(0,4)\n",
    "    else:\n",
    "        Q_value=model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiance(batch_size):\n",
    "    index=np.random.randint(len(replay_buffer),size=batch_size)\n",
    "    batch=[replay_buffer[i] for i in index]\n",
    "    states,actions,rewards,next_states,dones=[np.array([experience[field_index] for experience in batch]) for field_index in range(5)]\n",
    "    return states,actions,rewards,next_states,dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env,state,epsilon):\n",
    "    action=epsilon_greedy_policy(state,epsilon)\n",
    "    next_state,reward,done,info=env.step(action)\n",
    "    next_state=stack_frames(next_state,done)\n",
    "    replay_buffer.append((state,action,reward,next_state,done))\n",
    "    return next_state,reward,done,info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step():\n",
    "    states,actions,rewards,next_states,dones=sample_experiance(batch_size)\n",
    "    next_Q_value=model.predict(next_states)\n",
    "    max_next_Q_value=np.max(next_Q_value,axis=1)\n",
    "    target_Q_value=(rewards+(1-dones)*gamma*max_next_Q_value)\n",
    "    \n",
    "    mask=tf.one_hot(actions,output_shape)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_value=model(states)\n",
    "        Q_value=tf.reduce_sum(all_Q_value*mask,axis=1,keepdims=True)\n",
    "        loss=tf.reduce_mean(loss_fn(Q_value,target_Q_value))\n",
    "    grads=tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainig_loop(env):\n",
    "    for episode in tqdm(range(1,epochs)):\n",
    "        state=env.reset()\n",
    "        state=stack_frames(state,True)\n",
    "        for step in range(n_steps):\n",
    "            epsilon=max(1 - episode / 500, 0.1)\n",
    "            next_state,reward,done,info=play_one_step(env,state,epsilon)\n",
    "            if done:\n",
    "                break\n",
    "        if episode>20:\n",
    "            training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/999 [00:25<27:44,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainig_loop(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
